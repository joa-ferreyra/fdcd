# Unidad 5 - Transformaci√≥n de datos

## Introducci√≥n

En esta unidad, exploraremos algunas t√©cnicas esenciales y avanzadas de preprocesamiento de datos, aplicables en el an√°lisis exploratorio de datos, y en la mejora de la eficiencia de los modelos de aprendizaje autom√°tico. Estos m√©todos nos permiten transformar los datos brutos en un formato m√°s √∫til y comprensible, facilitando la extracci√≥n de patrones significativos y la generaci√≥n de predicciones m√°s precisas.

Comenzaremos discutiendo la estandarizaci√≥n y la normalizaci√≥n, dos t√©cnicas fundamentales para re-escalar los datos. Hablaremos sobre el m√©todo min-max y la transformaci√≥n de rango 0-1, que son formas populares de normalizaci√≥n para garantizar que todas las caracter√≠sticas est√©n en la misma escala. Adem√°s, exploraremos c√≥mo y cu√°ndo utilizar la transformaci√≥n logar√≠tmica para reducir la asimetr√≠a en los datos y c√≥mo la expansi√≥n en funciones base puede ayudar a modelar relaciones no lineales.

A continuaci√≥n, abordaremos la dicotomizaci√≥n de variables y la codificaci√≥n de variables categ√≥ricas. Estos conceptos son esenciales cuando trabajamos con datos no num√©ricos o cuando queremos transformar variables continuas en variables binarias. Examinaremos varias t√©cnicas de codificaci√≥n y discutiremos sus ventajas y desventajas.

Despu√©s, nos centraremos en las operaciones con datos temporales. Estudiaremos c√≥mo el suavizado y la media m√≥vil pueden ayudar a manejar la volatilidad en los datos de series temporales y a resaltar las tendencias subyacentes. Estas t√©cnicas son especialmente √∫tiles en campos como el an√°lisis financiero y el pron√≥stico del tiempo.

Finalmente, exploraremos las operaciones con datos de texto. La tokenizaci√≥n, es un proceso fundamental en el procesamiento del lenguaje natural. Veremos c√≥mo esta t√©cnica nos permite descomponer textos en unidades m√°s peque√±as (tokens), permitiendo un an√°lisis m√°s detallado.

## Estandarizaci√≥n, min-max, rango 0-1, logaritmo

La normalizaci√≥n de variables es importante en el aprendizaje autom√°tico y la estad√≠stica.  El √©xito de un algoritmo de aprendizaje autom√°tico depende en gran medida de la calidad de los datos que se alimentan al modelo. Los datos del mundo real a menudo est√°n sucios y contienen valores at√≠picos, valores faltantes, tipos de datos incorrectos, caracter√≠sticas irrelevantes o datos no estandarizados. La presencia de cualquiera de estos impedir√° que el modelo de aprendizaje autom√°tico aprenda adecuadamente. Por esta raz√≥n, transformar los datos brutos en un formato √∫til es una etapa esencial en el proceso de aprendizaje autom√°tico. Una t√©cnica que encontrar√°s m√∫ltiples veces al pre-procesar los datos es la normalizaci√≥n.

La normalizaci√≥n consiste en transformar las columnas num√©ricas a una escala com√∫n. En el aprendizaje autom√°tico, algunos valores de las caracter√≠sticas difieren de otros en varias veces su magnitud. Las caracter√≠sticas con valores m√°s altos dominar√°n el proceso de aprendizaje. Sin embargo, esto no significa que esas variables sean m√°s importantes para predecir el resultado del modelo. La normalizaci√≥n de datos transforma los datos de m√∫ltiples escalas a la misma escala. Despu√©s de la normalizaci√≥n, todas las variables tienen una influencia similar en el modelo, mejorando la estabilidad y el rendimiento del algoritmo de aprendizaje.

Existen m√∫ltiples t√©cnicas de normalizaci√≥n en estad√≠stica. En este art√≠culo, cubriremos las m√°s importantes:

- La escala m√°xima absoluta (**absolute maximum)**
- La escala de caracter√≠sticas m√≠n-m√°x
- El m√©todo de la puntuaci√≥n Z (**Z-score)**
- La estandarizaci√≥n robusta (**robust standardization)**

![Untitled](./imagenes/Untitled.png)

Adem√°s, explicaremos c√≥mo implementarlos con Pandas y Scikit-Learn.

El siguiente marco de datos contiene las entradas (variables independientes) de un modelo de regresi√≥n para predecir el precio de un auto usado: (1) la lectura del od√≥metro (km) y (2) la econom√≠a de combustible (km/l). En este art√≠culo, utilizamos un peque√±o conjunto de datos con fines de aprendizaje. Sin embargo, en el mundo real, los conjuntos de datos empleados ser√°n mucho m√°s grandes.

```python
import pandas as pd

# data frame containing the odometer reading (km) and the fuel economy (km/l) of second-hand cars
df_cars = pd.DataFrame([[120000, 11], [250000, 11.5], [175000, 15.8], [350000, 17], [400000, 10]],
                       columns=['odometer_reading', 'fuel_economy'])

df_cars
```

El resultado es el siguiente:

![Untitled](./imagenes/Untitled1.png)

Si realizamos la gr√°fica de los datos, obtendremos:

![Untitled](./imagenes/Untitled2.png)

Como se puede observar, la lectura del od√≥metro var√≠a de 120000 a 400000, mientras que la econom√≠a de combustible var√≠a de 10 a 17. Un modelo predictivo de regresi√≥n lineal m√∫ltiple ponderar√° m√°s la variable de lectura del od√≥metro, que el atributo de econom√≠a de combustible, debido a sus valores m√°s altos. Sin embargo, esto no significa que el atributo de lectura del od√≥metro sea m√°s importante como predictor. Para resolver este problema, tenemos que normalizar los valores de ambas variables.

### La escala m√°xima absoluta

La escala m√°xima absoluta vuelve a escalar cada caracter√≠stica entre -1 y 1 dividiendo cada observaci√≥n por su valor absoluto m√°ximo.

![Untitled](./imagenes/Untitled3.png)

Podemos aplicar el escalado absoluto m√°ximo en Pandas usando los m√©todos .max() y .abs(), como se muestra a continuaci√≥n.

```python
# apply the maximum absolute scaling in Pandas using the .abs() and .max() methods
def maximum_absolute_scaling(df):
    # copy the dataframe
    df_scaled = df.copy()
    # apply maximum absolute scaling
    for column in df_scaled.columns:
        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()
    return df_scaled
    
# call the maximum_absolute_scaling function
df_cars_scaled = maximum_absolute_scaling(df_cars)

df_cars_scaled
```

Y el resultado obtenido es:

![Untitled](./imagenes/Untitled4.png)

Alternativamente, podemos usar la biblioteca Scikit-learn para calcular la escala absoluta m√°xima. Primero, creamos un objeto abs_scaler con la clase `MaxAbsScaler`. Luego, usamos el m√©todo de ajuste (`fit`) para conocer los par√°metros necesarios para escalar los datos (el valor absoluto m√°ximo de cada caracter√≠stica). Finalmente, transformamos los datos usando esos par√°metros.

```python
from sklearn.preprocessing import MaxAbsScaler

# create an abs_scaler object
abs_scaler = MaxAbsScaler()

# calculate the maximum absolute value for scaling the data using the fit method
abs_scaler.fit(df_cars)

# the maximum absolute values calculated by the fit method
abs_scaler.max_abs_
# array([4.0e+05, 1.7e+01])

# transform the data using the parameters calculated by the fit method (the maximum absolute values)
scaled_data = abs_scaler.transform(df_cars)

# store the results in a data frame
df_scaled = pd.DataFrame(scaled_data, columns=df_cars.columns)

# visualize the data frame
df_scaled
```

![Untitled](./imagenes/Untitled5.png)

Como se puede observar, obtenemos los mismos resultados utilizando Pandas y Scikit-learn. El siguiente gr√°fico muestra los datos transformados despu√©s de realizar el escalado absoluto m√°ximo.

![Untitled](./imagenes/Untitled6.png)

<aside>
üí° Es importante destacar que esta t√©cnica es √∫til cuando los datos tienen valores tanto positivos como negativos y se desea mantener esta distinci√≥n despu√©s de la normalizaci√≥n.

</aside>

### La normalizaci√≥n m√≠n-m√°x

El enfoque min-max (a menudo llamado normalizaci√≥n) vuelve a escalar la caracter√≠stica a un rango fijo de [0,1] al restar el valor m√≠nimo de la caracter√≠stica y luego dividirlo por el rango.

$$
X_{\text{norm}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
$$

Podemos aplicar la escala min-max en Pandas usando los m√©todos .min() y .max().

```python
# apply the min-max scaling in Pandas using the .min() and .max() methods
def min_max_scaling(df):
    # copy the dataframe
    df_norm = df.copy()
    # apply min-max scaling
    for column in df_norm.columns:
        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())
        
    return df_norm
    
# call the min_max_scaling function
df_cars_normalized = min_max_scaling(df_cars)

df_cars_normalized
```

![Untitled](./imagenes/Untitled7.png)

Alternativamente, podemos usar la clase `MinMaxScaler` disponible en la biblioteca Scikit-learn. Primero, creamos un objeto escalador. Luego, ajustamos los par√°metros del escalador, lo que significa que calculamos el valor m√≠nimo y m√°ximo para cada funci√≥n. Finalmente, transformamos los datos usando esos par√°metros.

```python
from sklearn.preprocessing import MinMaxScaler

# create a scaler object
scaler = MinMaxScaler()
# fit and transform the data
df_norm = pd.DataFrame(scaler.fit_transform(df_cars), columns=df_cars.columns)

df_norm
```

![Untitled](./imagenes/Untitled8.png)

Adem√°s, podemos obtener los valores m√≠nimos y m√°ximos calculados por la funci√≥n de ajuste para normalizar los datos con los atributos data_min_ y data_max_.

```python
# minimum values for normalizing the data
scaler.data_min_
# array([1.2e+05, 1.0e+01])

# maximum values for normalizing the data
scaler.data_max_
# array([4.0e+05, 1.7e+01])
```

La siguiente gr√°fica muestra los datos despu√©s de aplicar la escala de caracter√≠sticas min-max. Como puede observar, esta t√©cnica de normalizaci√≥n vuelve a escalar todos los valores de caracter√≠sticas para que est√©n dentro del rango de [0, 1].

![Untitled](./imagenes/Untitled9.png)

Como puede observar, obtenemos los mismos resultados utilizando Pandas y Scikit-learn. Sin embargo, si desea realizar muchos pasos de transformaci√≥n de datos, se recomienda utilizar `MinMaxScaler` como entrada en un constructor de Pipeline en lugar de realizar la normalizaci√≥n con Pandas.

Adem√°s, es importante tener en cuenta que el escalado absoluto m√°ximo y el escalado m√≠nimo-m√°ximo son muy sensibles a los valores at√≠picos porque un solo valor at√≠pico puede influir en los valores m√≠nimo y m√°ximo y tener un gran efecto en los resultados.

### El m√©todo de puntaje z

El m√©todo de puntuaci√≥n z (a menudo llamado estandarizaci√≥n) transforma los datos en una distribuci√≥n con una media de 0 y una desviaci√≥n est√°ndar de 1. Cada valor estandarizado se calcula restando la media de la caracter√≠stica correspondiente y luego dividiendo por la desviaci√≥n est√°ndar.

$$
z = \frac{X - \mu}{\sigma}
$$

A diferencia de la escala m√≠nima-m√°xima, la puntuaci√≥n z no vuelve a escalar la caracter√≠stica a un rango fijo. La puntuaci√≥n z suele oscilar entre -3,00 y 3,00 (m√°s del 99 % de los datos) si la entrada tiene una distribuci√≥n normal. Sin embargo, los valores estandarizados tambi√©n pueden ser m√°s altos o m√°s bajos, como se muestra en la imagen a continuaci√≥n.

![Untitled](./imagenes/Untitled10.png)

Es importante tener en cuenta que las puntuaciones z no tienen necesariamente una distribuci√≥n normal. Simplemente escalan los datos y siguen la misma distribuci√≥n que la entrada original. Esta distribuci√≥n transformada tiene una media de 0 y una desviaci√≥n est√°ndar de 1 y ser√° la distribuci√≥n normal est√°ndar (vea la imagen de arriba) solo si la caracter√≠stica de entrada sigue una distribuci√≥n normal.

Podemos calcular el puntaje z en Pandas usando los m√©todos .mean() y std().

```python
# apply the z-score method in Pandas using the .mean() and .std() methods
def z_score(df):
    # copy the dataframe
    df_std = df.copy()
    # apply the z-score method
    for column in df_std.columns:
        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()
        
    return df_std
    
# call the z_score function
df_cars_standardized = z_score(df_cars)

df_cars_standardized
```

![Untitled](./imagenes/Untitled11.png)

Alternativamente, podemos usar la clase `StandardScaler` disponible en la biblioteca Scikit-learn para realizar el puntaje z. Primero, creamos un objeto standard_scaler. Luego, calculamos los par√°metros de la transformaci√≥n (en este caso la media y la desviaci√≥n est√°ndar) usando el m√©todo .fit(). A continuaci√≥n, llamamos al m√©todo .transform() para aplicar la estandarizaci√≥n al marco de datos. El m√©todo .transform() utiliza los par√°metros generados a partir del m√©todo .fit() para realizar la puntuaci√≥n z.

```python
from sklearn.preprocessing import StandardScaler

# create a scaler object
std_scaler = StandardScaler()
std_scaler
# fit and transform the data
df_std = pd.DataFrame(std_scaler.fit_transform(df_cars), columns=df_cars.columns)

df_std
```

![Untitled](./imagenes/Untitled12.png)

Para simplificar el c√≥digo, hemos utilizado el m√©todo .fit_transform() que combina ambos m√©todos (ajuste y transformaci√≥n).

Como puede observar, los resultados difieren de los obtenidos con Pandas. La funci√≥n `StandardScaler` calcula la desviaci√≥n est√°ndar de la poblaci√≥n donde la suma de los cuadrados se divide por N (n√∫mero de valores en la poblaci√≥n).

![Untitled](./imagenes/Untitled13.png)

Por el contrario, el m√©todo .std() calcula la desviaci√≥n est√°ndar de la muestra donde el denominador de la f√≥rmula es N-1 en lugar de N.

![Untitled](./imagenes/Untitled14.png)

Para obtener los mismos resultados con Pandas, establecemos el par√°metro ddof igual a 0 (el valor predeterminado es ddof=1) que representa el divisor utilizado en los c√°lculos (N-ddof).

```python
# population standard deviation with Pandas
df_cars.std(ddof=0)
```

![Untitled](./imagenes/Untitled15.png)

Podemos obtener los par√°metros calculados por la funci√≥n de ajuste para estandarizar los datos con los atributos mean_ y scale_. Como puede observar, obtenemos los mismos resultados en Scikit-learn y Pandas al establecer el par√°metro ddof igual a 0 en el m√©todo .std().

```python
# standard deviation for standardizing the data
std_scaler.scale_
# array([1.04517941e+05, 2.79542483e+00])

# mean for standardizing the data
std_scaler.mean_
# array([2.590e+05, 1.306e+01])
```

La siguiente gr√°fica muestra los datos despu√©s de aplicar el m√©todo de puntuaci√≥n z que se calcula utilizando la desviaci√≥n est√°ndar de la poblaci√≥n (dividida por N).

![Untitled](./imagenes/Untitled16.png)

La normalizaci√≥n de puntaje Z, es una t√©cnica com√∫nmente utilizada en estad√≠stica y ciencia de datos por varias razones:

- **Comparabilidad**: Permite comparar medidas que provienen de diferentes escalas o distribuciones. Por ejemplo, si tienes dos conjuntos de datos con diferentes escalas, como la estatura en cent√≠metros y el peso en kilogramos, puedes usar el puntaje z para estandarizarlos y hacerlos comparables.
- **Manejo de outliers**: Los puntajes z pueden ser √∫tiles para identificar y manejar valores at√≠picos (outliers). Por lo general, se considera que un valor es un outlier si su puntaje z es mayor que 3 o menor que -3.
- **Requisito para ciertos algoritmos de aprendizaje autom√°tico**: Algunos algoritmos de aprendizaje autom√°tico asumen que todas las caracter√≠sticas tienen la misma escala y distribuci√≥n. Por lo tanto, se utiliza la normalizaci√≥n del puntaje z para cumplir con este requisito. Por ejemplo, en algoritmos de aprendizaje autom√°tico como k-nearest neighbors (k-NN), support vector machines (SVM) y redes neuronales, donde la escala de las caracter√≠sticas puede afectar a los resultados del modelo.
- **Componentes Principales (PCA)**: En el an√°lisis de componentes principales, que es una t√©cnica de reducci√≥n de la dimensionalidad, se recomienda estandarizar las caracter√≠sticas para garantizar que todas ellas tengan el mismo peso.

### El escalado robusto

En el escalado robusto, escalamos cada caracter√≠stica del conjunto de datos restando la mediana y luego dividiendo por el rango intercuart√≠lico. El rango intercuart√≠lico (RIC) se define como la diferencia entre el tercer y el primer cuartil y representa el 50% central de los datos. Matem√°ticamente, el escalador robusto se puede expresar como:

![Untitled](./imagenes/Untitled17.png)

donde Q1(x) es el primer cuartil del atributo x, Q2(x) es la mediana y Q3(x) es el tercer cuartil.

Este m√©todo es √∫til cuando se trabaja con conjuntos de datos que contienen muchos valores at√≠picos porque utiliza estad√≠sticas que son s√≥lidas para los valores at√≠picos (mediana y rango intercuart√≠lico), en contraste con los escaladores anteriores, que utilizan estad√≠sticas que se ven muy afectadas por los valores at√≠picos como el m√°ximo, el m√≠nimo, la media y la desviaci√≥n est√°ndar.

Veamos c√≥mo los valores at√≠picos afectan los resultados despu√©s de escalar los datos con escalado m√≠nimo-m√°ximo y escalado robusto.

![Untitled](./imagenes/Untitled18.png)

Otro ejemplo: El siguiente conjunto de datos contiene 10 puntos de datos, uno de ellos es un valor at√≠pico (variable1 = 30).

```python
# the data frame contains one outlier
df_data = pd.DataFrame({'variable1':[1,2,3,4,5,6,7,30], 'variable2':[1,2,3,4,5,6,7,8]})

df_data
```

![Untitled](./imagenes/Untitled19.png)

La escala min-max desplaza la variable 1 hacia 0 debido a la presencia de un valor at√≠pico en comparaci√≥n con la variable 2 donde los puntos se distribuyen uniformemente en un rango de 0 a 1.

![Untitled](./imagenes/Untitled20.png)

```python
# scatter plot of the data after applying min-max scaling
sns.scatterplot(x='variable1', y='variable2', data=df_min_max, s=100, color='blue')

# xticks and yticks
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# labels and title
plt.xlabel('variable1', fontsize=14)
plt.ylabel('variable2', fontsize=14)
plt.title('The min-max scaling', fontsize=20)
```

![Untitled](./imagenes/Untitled21.png)

Antes de escalar, el primer punto de datos tiene un valor de (1,1), tanto la variable 1 como la variable 2 tienen valores iguales. Una vez transformado, el valor de la variable 2 es mucho mayor que el de la variable 1 (0,034, 0,142). Esto se debe a que la variable 1 tiene un valor at√≠pico.

Por el contrario, si aplicamos un escalado robusto, ambas variables tienen los mismos valores (-1.00,-1.00) despu√©s de la transformaci√≥n, ya que ambas caracter√≠sticas tienen la misma mediana y rango intercuart√≠lico, siendo el valor at√≠pico el que se desplaza.

![Untitled](./imagenes/Untitled22.png)

```python
# scatter plot of the data after applying min-max scaling
sns.scatterplot(x='variable1', y='variable2', data=df_robust, s=100, color='salmon')

# xticks and yticks
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# labels and title
plt.xlabel('variable1', fontsize=14)
plt.ylabel('variable2', fontsize=14)
plt.title('The robust scaling', fontsize=20)
```

![Untitled](./imagenes/Untitled23.png)

Ahora es el momento de aplicar la escala robusta al conjunto de datos de autom√≥viles.
Como hicimos anteriormente, podemos realizar un escalado robusto usando Pandas.

```python
# apply the robust scaling in Pandas using the .median() and .quantile() methods
def robust_scaling(df):
    # copy the dataframe
    df_robust = df.copy()
    # apply robust scaling
    for column in df_robust.columns:
        df_robust[column] = (df_robust[column] - df_robust[column].median())  / (df_robust[column].quantile(0.75) - df_robust[column].quantile(0.25))
    return df_robust
    
# call the robust_scaling function
df_cars_robust = robust_scaling(df_cars)

df_cars_robust
```

![Untitled](./imagenes/Untitled24.png)

La mediana se define como el punto medio de la distribuci√≥n, lo que significa que el 50% de los valores de la distribuci√≥n son m√°s peque√±os que la mediana. En Pandas, podemos calcularlo con los m√©todos .median() o .quantile(0.5). El primer cuartil es la mediana de la mitad inferior del conjunto de datos (el 25 % de los valores se encuentran por debajo del primer cuartil) y se puede calcular con el m√©todo .quantile(0.25). El tercer cuartil representa la mediana de la mitad superior del conjunto de datos (75 % de los valores se encuentran por debajo del tercer cuartil) y se puede calcular con el m√©todo .quantile(0.75).

Como alternativa a Pandas, tambi√©n podemos realizar un escalado robusto utilizando la biblioteca Scikit-learn.

```python
from sklearn.preprocessing import RobustScaler

# create a scaler object
scaler = RobustScaler()
# fit and transform the data
df_robust = pd.DataFrame(scaler.fit_transform(df_cars), columns=df_cars.columns)

df_robust
```

![Untitled](./imagenes/Untitled25.png)

Como se muestra arriba, obtenemos los mismos resultados que antes.

El siguiente gr√°fico muestra los resultados despu√©s de transformar los datos con un escalado robusto.

![Untitled](./imagenes/Untitled26.png)

### Resumen

La normalizaci√≥n de datos consiste en transformar columnas num√©ricas a una escala com√∫n. En Python podemos implementar la normalizaci√≥n de datos de una forma muy sencilla. La biblioteca de Pandas contiene m√∫ltiples m√©todos integrados para calcular las funciones estad√≠sticas descriptivas m√°s comunes que hacen que las t√©cnicas de normalizaci√≥n de datos sean realmente f√°ciles de implementar. Como otra opci√≥n, podemos usar la biblioteca Scikit-Learn para transformar los datos en una escala com√∫n. En esta biblioteca ya est√°n implementados los m√©todos de escalado m√°s frecuentes.

Adem√°s de la normalizaci√≥n de datos, existen m√∫ltiples t√©cnicas de preprocesamiento de datos que debemos aplicar para garantizar el rendimiento del algoritmo de aprendizaje. 

### Transformaci√≥n Logar√≠tmica

La transformaci√≥n logar√≠tmica es una t√©cnica de transformaci√≥n de datos com√∫nmente utilizada en estad√≠sticas, ciencia de datos y aprendizaje autom√°tico. Es una operaci√≥n matem√°tica que toma un n√∫mero y devuelve el logaritmo de ese n√∫mero, generalmente utilizando la base 10 o la base e (el n√∫mero de Euler, aproximadamente igual a 2.71828). Resulta particularmente √∫til para tratar con datos que abarcan varios √≥rdenes de magnitud. Puede ayudar a manejar varias situaciones, como por ejemplo:

- **Sesgo o asimetr√≠a en la distribuci√≥n**: Cuando los datos est√°n sesgados, la transformaci√≥n logar√≠tmica puede ayudar a hacer que los datos sean m√°s sim√©tricos y, por lo tanto, m√°s manejables para el an√°lisis. Esto puede hacer que los supuestos de muchos modelos estad√≠sticos (como la normalidad) sean m√°s apropiados.
- **Relaciones no lineales**: Si la relaci√≥n entre dos variables es exponencial en lugar de lineal, tomar el logaritmo de una o ambas variables puede convertir la relaci√≥n en una relaci√≥n lineal, lo que facilita su an√°lisis.
- **Manejo de grandes rangos de valores**: En muchos conjuntos de datos, algunos valores pueden ser muy grandes en comparaci√≥n con otros. En estos casos, la transformaci√≥n logar√≠tmica puede comprimir los valores m√°s grandes m√°s que los peque√±os, lo que permite un manejo m√°s f√°cil de los datos.

<aside>
üí° Es importante tener en cuenta que la transformaci√≥n logar√≠tmica solo puede aplicarse a n√∫meros positivos.

</aside>

Para entender mejor estos conceptos, es necesario entender qu√© es una distribuci√≥n normal y la funci√≥n logar√≠tmica.

**¬øQu√© es una distribuci√≥n normal?**
Antes de entrar en la transformaci√≥n de registros, hablemos r√°pidamente sobre la distribuci√≥n normal. La distribuci√≥n normal es un concepto estad√≠stico y de probabilidad ampliamente utilizado en estudios cient√≠ficos por sus muchos beneficios. Solo por nombrar algunos de estos beneficios, la distribuci√≥n normal es simple. Su media, mediana y moda tienen el mismo valor y se puede definir con solo dos par√°metros: media y varianza. Tambi√©n tiene importantes implicaciones matem√°ticas como el Teorema del L√≠mite Central.

![Distribuci√≥n normal](./imagenes/Untitled27.png)

Distribuci√≥n normal

Desafortunadamente, nuestros conjuntos de datos de la vida real no siempre siguen la distribuci√≥n normal. A menudo son tan sesgados que invalidan los resultados de nuestros an√°lisis estad√≠sticos. Ah√≠ es donde entra Transformaci√≥n Logar√≠tmica.

**¬øQu√© es el logaritmo?**
Los logaritmos son herramientas esenciales en el modelado estad√≠stico y el an√°lisis estad√≠stico. Se puede definir un logaritmo con respecto a una base (b) donde el logaritmo en base b de X es igual a y porque X es igual a b elevado a y (log(X) =y porque X = b  ∏). Puede tomar cualquier n√∫mero positivo como la base del logaritmo, pero las bases m√°s utilizadas son:

Base 2: el logaritmo en base 2 de 8 es 3, porque 2¬≥ = 8
Base 10: el logaritmo en base 10 de 100 es 2, porque 10¬≤ = 100
Logaritmo natural: la base del logaritmo natural es la constante matem√°tica "e" o el n√∫mero de Euler, que es igual a 2,718282. Entonces, el logaritmo natural de 7,389 es 2, porque e¬≤ = 7,389.

![Untitled](./imagenes/Untitled28.png)

**¬øQu√© es la transformaci√≥n de registros?**
La transformaci√≥n de registro es un m√©todo de transformaci√≥n de datos en el que reemplaza cada variable x con un registro (x). La elecci√≥n de la base del logaritmo generalmente se deja en manos del analista y depender√≠a de los prop√≥sitos del modelado estad√≠stico. Por ahora, nos centraremos en la transformaci√≥n del logaritmo natural. El registro de la naturaleza se denota como ln.

Cuando nuestros datos continuos originales no siguen la curva de campana, podemos transformar estos datos para que sean lo m√°s "normales" posibles, de modo que los resultados del an√°lisis estad√≠stico de estos datos sean m√°s v√°lidos. En otras palabras, la transformaci√≥n de registro reduce o elimina la asimetr√≠a de nuestros datos originales. La advertencia importante aqu√≠ es que los datos originales deben si o si seguir aproximadamente una distribuci√≥n logar√≠tmica normal. De lo contrario, la transformaci√≥n de registro no funcionar√°.

![Izquierda: Distribuci√≥n de la muestra original. Derecha: Distribuci√≥n luego de la transformaci√≥n logar√≠tmica.](./imagenes/Untitled29.png)

Izquierda: Distribuci√≥n de la muestra original. Derecha: Distribuci√≥n luego de la transformaci√≥n logar√≠tmica.

Para ver un ejemplo, usaremos un dataset de precios de viviendas del condado de King (publicado [aqu√≠](https://www.kaggle.com/code/gabriellima/house-sales-in-king-county-usa/input)) como ejemplo. Graficaremos las distribuciones antes y despu√©s de la transformaci√≥n.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

df = pd.read_csv("kc_house_data.csv")

df['price_log'] = np.log(df['price'])

sns.distplot(df['price_log'])

fig = plt.figure()
```

Los resultados obtenidos son:

![Dataset original](./imagenes/Untitled30.png)

Dataset original

![Precios transformados logar√≠tmicamente](./imagenes/Untitled31.png)

Precios transformados logar√≠tmicamente

Es importante tener en cuenta que la transformaci√≥n logar√≠tmica solo debe aplicarse a datos positivos, ya que el logaritmo natural no est√° definido para valores menores o iguales a cero. Adem√°s, es importante tener en cuenta que la transformaci√≥n del registro no hace que los datos sean normales, solo reduce la asimetr√≠a.

## Dicotomizaci√≥n de variables, codificaci√≥n de variables categ√≥ricas

### **Dicotomizaci√≥n (Dichotomization)**

La Dicotomizaci√≥n consiste en tomar una variable y dividirla en dos grupos, a menudo bas√°ndose en un punto de corte. Por ejemplo, podr√≠as tener una variable continua, como la edad, y podr√≠as querer dividirla en dos grupos: aquellos menores de 30 y aquellos de 30 o m√°s. Estos grupos se convierten luego en categor√≠as binarias: 0 (menor de 30) y 1 (30 o m√°s). La dicotomizaci√≥n se utiliza a menudo cuando queremos convertir una variable continua en una variable categ√≥rica binaria para su an√°lisis.

Este proceso a menudo implica establecer un punto de corte y asignar categor√≠as seg√∫n si los valores est√°n por encima o por debajo de este punto.

Supongamos que tienes un DataFrame de pandas **`df`** con una columna "Edad" y deseas dicotomizarla en dos grupos: menores de 18 y 18 o m√°s. Aqu√≠ mostramos c√≥mo hacerlo:

```python
import pandas as pd

# Supongamos que este es tu DataFrame original
df = pd.DataFrame({
    'Nombre': ['Ana', 'Juan', 'Pedro', 'Maria', 'Luis', 'Elena'],
    'Edad': [14, 22, 18, 35, 16, 20]
})

# Creamos una nueva columna "Mayor_de_18" utilizando la funci√≥n where de numpy
df['Mayor_de_18'] = pd.np.where(df['Edad'] >= 18, 1, 0)

df
```

Obtendremos el siguiente DataFrame:

![Untitled](./imagenes/Untitled32.png)

### **Binning (**o "bucketing")

Esta es una t√©cnica m√°s general que la dicotomizaci√≥n. En lugar de dividir los datos en dos grupos, los datos se dividen en varios grupos o "bins" o ‚Äúbuckets‚Äù. Por ejemplo, podr√≠as tomar una variable continua, como la edad, y dividirla en varios grupos: menores de 18, de 18 a 24, de 25 a 34, de 35 a 44, etc. Al igual que con la dicotomizaci√≥n, los grupos se convierten en categor√≠as. El binning puede ser √∫til por varias razones:

- **Reducci√≥n de ruido**: El bucketing puede suavizar las fluctuaciones menores en los datos que podr√≠an ser ruido aleatorio en lugar de se√±ales significativas.
- **Manejo de valores at√≠picos**: Al agrupar los datos en buckets, los valores extremadamente altos o bajos (valores at√≠picos) se colocan en el bucket superior o inferior, lo que puede hacer que los modelos sean m√°s robustos a estos valores at√≠picos.
- **Conversi√≥n de variables continuas en categ√≥ricas**: Algunas t√©cnicas de modelado funcionan mejor con variables categ√≥ricas que con variables continuas. El bucketing permite convertir variables continuas en categ√≥ricas.
- **Facilita la interpretaci√≥n**: Al agrupar los valores en categor√≠as m√°s amplias, puede facilitar la interpretaci√≥n de los resultados, especialmente en el caso de variables con un amplio rango de valores.

El bucketing debe usarse con cuidado, ya que tambi√©n tiene desventajas. Puede llevar a una p√©rdida de informaci√≥n, ya que los valores dentro de cada bucket se tratan de la misma manera, aunque pueden ser bastante diferentes. Adem√°s, el rendimiento del modelo puede ser muy sensible a c√≥mo se definen los buckets.

El binning o "bucketing" en pandas se puede realizar utilizando varias funciones, como **`cut`** y **`qcut`**. Aqu√≠ muestramos c√≥mo hacerlo:

```python
import pandas as pd

# Supongamos que este es tu DataFrame original
df = pd.DataFrame({
    'Nombre': ['Ana', 'Juan', 'Pedro', 'Maria', 'Luis', 'Elena'],
    'Edad': [14, 22, 18, 35, 16, 20]
})

# Creas una nueva columna "Grupo_Edad" utilizando la funci√≥n cut de pandas
bins = [0, 18, 30, 100]  # Define los rangos de edad que desees
labels = ['Menor de 18', '18-30', 'Mayor de 30']  # Define las etiquetas para cada bin
df['Grupo_Edad'] = pd.cut(df['Edad'], bins=bins, labels=labels)

df
```

Obtendremos el siguiente DataFrame:

![Untitled](./imagenes/Untitled33.png)

### Codificaci√≥n de variables categ√≥ricas

La mayor√≠a de los algoritmos de aprendizaje autom√°tico no pueden manejar variables categ√≥ricas a menos que las convirtamos en valores num√©ricos. El rendimiento de muchos algoritmos var√≠a en funci√≥n de c√≥mo se codifican las variables categ√≥ricas.

Las variables categ√≥ricas se pueden dividir en dos categor√≠as: nominales (sin orden particular) y ordinales (algunas ordenadas).

![Untitled](./imagenes/Untitled34.png)

Algunos ejemplos como a continuaci√≥n para la variable Nominal:

- Rojo, amarillo, rosa, azul
- Singapur, Jap√≥n, Estados Unidos, India, Corea
- Vaca, Perro, Gato, Serpiente

Ejemplo de variables ordinales:

- Alto, medio, bajo
- "Totalmente de acuerdo", de acuerdo, neutral, en desacuerdo, y "totalmente en desacuerdo".
- Excelente, Bien, Malo

Hay muchas maneras en que podemos codificar estas variables categ√≥ricas como n√∫meros y usarlas en un algoritmo. Veremos algunos de m√©todos de codificaci√≥n:

1. Codificaci√≥n activa
2. Codificaci√≥n de etiquetas
3. Codificaci√≥n ordinal
4. Codificaci√≥n Helmert
5. Codificaci√≥n binaria

Para la explicaci√≥n, usaremos este marco de datos, que tiene dos variables o caracter√≠sticas independientes (Temperatura y Color) y una etiqueta (Objetivo). Tambi√©n tiene Rec-No, que es un n√∫mero de secuencia del registro. Hay un total de 10 registros en este marco de datos. El c√≥digo de Python se ver√≠a como se muestra a continuaci√≥n.

![Untitled](./imagenes/Untitled35.png)

```python
import pandas as pd
import numpy as np

data = {'Temperature': ['Hot', 'Cold', 'Very Hot', 'Warm', 'Hot', 'Warm', 'Warm', 'Hot', 'Hot', 'Cold'],
         'Color': ['Red', 'Yellow', 'Blue', 'Blue', 'Red', 'Yellow', 'Red', 'Yellow', 'Yellow', 'Yellow'],
         'Target': ['1', '1', '1', '0', '1', '0', '1', '0', '1', '1']}

df = pd.DataFrame(data, columns=['Temperature', 'Color', 'Target'])

df
```

![Untitled](./imagenes/Untitled36.png)

Usaremos Pandas y Scikit-learn y [`category_encoders`](https://github.com/scikit-learn-contrib/category_encoders) (biblioteca de contribuci√≥n de Scikit-learn) para mostrar diferentes m√©todos de codificaci√≥n en Python.

**One Hot Encoding**
En este m√©todo, asignamos cada categor√≠a a un vector que contiene 1 y 0, lo que denota la presencia o ausencia de la caracter√≠stica. El n√∫mero de vectores depende del n√∫mero de categor√≠as de caracter√≠sticas. Este m√©todo produce muchas columnas que ralentizan significativamente el aprendizaje si el n√∫mero de la categor√≠a es muy alto para la funci√≥n. Pandas tiene la funci√≥n get_dummies, que es bastante f√°cil de usar. El c√≥digo del marco de datos de muestra ser√≠a el siguiente:

![Untitled](./imagenes/Untitled37.png)

Scikit-learn tiene OneHotEncoder para este prop√≥sito, pero no crea una columna de funci√≥n adicional (se necesita otro c√≥digo, como se muestra en el ejemplo de c√≥digo a continuaci√≥n).

![Untitled](./imagenes/Untitled38.png)

One Hot Encoding es muy popular. Podemos representar todas las categor√≠as por N-1 (N= Nro de Categor√≠a) como suficiente para codificar la que no est√° incluida. Por lo general, para la regresi√≥n, usamos N-1 (eliminar la primera o la √∫ltima columna de la nueva funci√≥n One Hot Coded). A√∫n as√≠, para la clasificaci√≥n, la recomendaci√≥n es usar todas las N columnas, ya que la mayor√≠a de los algoritmos basados en √°rboles construyen un √°rbol basado en todas las variables disponibles. Se debe usar One Hot Encoding con N-1 variables binarias en la regresi√≥n lineal para garantizar el n√∫mero correcto de grados de libertad (N-1). La regresi√≥n lineal tiene acceso a todas las funciones a medida que se entrena y, por lo tanto, examina todo el conjunto de variables ficticias. Esto significa que N-1 variables binarias brindan informaci√≥n completa sobre (representan completamente) la variable categ√≥rica original de la regresi√≥n lineal. Este enfoque se puede adoptar para cualquier algoritmo de aprendizaje autom√°tico que analice TODAS las caracter√≠sticas simult√°neamente durante el entrenamiento, por ejemplo, m√°quinas de vectores de soporte (SVM) y redes neuronales, as√≠ como algoritmos de clustering.

**Codificaci√≥n de etiquetas (Label Encoding)**
En esta codificaci√≥n, a cada categor√≠a se le asigna un valor de 1 a N (donde N es el n√∫mero de categor√≠as para la funci√≥n. Un problema importante con este enfoque es que no hay relaci√≥n ni orden entre estas clases, pero el algoritmo podr√≠a considerarlas como alg√∫n orden o alguna relaci√≥n En el siguiente ejemplo, puede verse como (Cold<Hot<Very Hot<Warm‚Ä¶.0 < 1 < 2 < 3) .Scikit-learn c√≥digo para el marco de datos de la siguiente manera:

![Untitled](./imagenes/Untitled39.png)

`factorize` de Pandas tambi√©n realizan la misma funci√≥n:

![Untitled](./imagenes/Untitled40.png)

**Codificaci√≥n ordinal**
Realizamos la codificaci√≥n ordinal para garantizar que la codificaci√≥n de las variables conserve la naturaleza ordinal de la variable. Esto es razonable solo para variables ordinales, como mencionamos al principio de este art√≠culo. Esta codificaci√≥n se ve casi similar a la codificaci√≥n de etiquetas, pero ligeramente diferente, ya que la codificaci√≥n de etiquetas no considerar√≠a si la variable es ordinal o no, y asignar√° una secuencia de n√∫meros enteros.

Seg√∫n el orden de los datos (Pandas asigna Hot (0), Cold (1), ‚ÄúVery Hot‚Äù (2) and Warm (3)) o seg√∫n el orden alfab√©tico (scikit-learn asigna Cold(0), Hot(1), ‚ÄúVery Hot‚Äù (2) and Warm (3)).
Si consideramos la escala de temperatura como el orden, entonces el valor ordinal deber√≠a ir de Cold a ‚ÄúVery Hot‚Äù. ‚Äú La codificaci√≥n ordinal asignar√° valores como ( Cold(1) <Warm(2)<Hot(3)<‚ÄùVery Hot(4)). Por lo general, la codificaci√≥n ordinal se realiza a partir de 1.

Consulte este c√≥digo usando Pandas, donde primero debemos asignar el orden original de la variable a trav√©s de un diccionario. Luego podemos mapear cada fila para la variable seg√∫n el diccionario.

![Untitled](./imagenes/Untitled41.png)

Aunque es muy sencillo, requiere codificaci√≥n para indicar los valores ordinales y la asignaci√≥n real de texto a un n√∫mero entero seg√∫n el orden.

**Codificaci√≥n [Helmert](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#HELMERT)**
La versi√≥n en `category_encoders` a veces se denomina Codificaci√≥n Helmert inversa. En esta codificaci√≥n, la media de la variable dependiente de un nivel se compara con la media de la variable dependiente de todos los niveles anteriores. Por lo tanto, el nombre 'inverso' se usa para diferenciar la codificaci√≥n directa de Helmert.

Esta codificaci√≥n contrasta cada nivel de una variable categ√≥rica con el promedio de los niveles siguientes. Esta es una forma de codificaci√≥n "contraste", que compara cada nivel de la variable categ√≥rica con un est√°ndar de alg√∫n tipo.

![Untitled](./imagenes/Untitled42.png)

**Codificaci√≥n binaria**
La codificaci√≥n binaria convierte una categor√≠a en d√≠gitos binarios. Cada d√≠gito binario crea una columna de caracter√≠sticas. Si hay n categor√≠as √∫nicas, la codificaci√≥n binaria da como resultado las √∫nicas funciones de registro (base 2)‚Åø. En este ejemplo, tenemos cuatro caracter√≠sticas; por lo tanto, las caracter√≠sticas codificadas en binario ser√°n tres caracter√≠sticas. En comparaci√≥n con One Hot Encoding, esto requerir√° menos columnas de funciones (para 100 categor√≠as, One Hot Encoding tendr√° 100 funciones, mientras que para la codificaci√≥n binaria, necesitaremos solo siete funciones).

Para la codificaci√≥n binaria, se deben seguir los siguientes pasos:

Las categor√≠as se convierten primero en orden num√©rico a partir de 1 (el orden se crea a medida que las categor√≠as aparecen en un conjunto de datos y no significan ninguna naturaleza ordinal)
Luego, esos n√∫meros enteros se convierten en c√≥digo binario, por ejemplo, 3 se convierte en 011, 4 se convierte en 100
Luego, los d√≠gitos del n√∫mero binario forman columnas separadas.
Consulte el siguiente diagrama para una mejor intuici√≥n.

![Untitled](./imagenes/Untitled43.png)

Usaremos el paquete `category_encoders` para esto, y el nombre de la funci√≥n es BinaryEncoder.

![Untitled](./imagenes/Untitled44.png)

## Operaciones con datos temporales

### Suavizado de variables temporales

El suavizado de variables temporales es una t√©cnica estad√≠stica de transformaci√≥n de datos, que se utiliza en el an√°lisis de series de tiempo. Se utiliza para eliminar el ruido y las fluctuaciones aleatorias en los datos, y para revelar patrones subyacentes como las tendencias y la estacionalidad. Al suavizar los datos, se puede obtener una representaci√≥n m√°s clara de lo que realmente est√° sucediendo.
Suavizar una serie temporal elimina ciertas frecuencias o componentes para obtener una vista de la estructura subyacente de la serie temporal. Por ejemplo, queremos eliminar el ruido para enfatizar la se√±al en la serie temporal antes de comenzar nuestro an√°lisis. Eliminar el ruido mejora la calidad de nuestros datos y, a su vez, los resultados de nuestro an√°lisis. Piensa en las famosas palabras "basura que entra, basura que sale".

El suavizado de datos b√°sicamente no es m√°s que promediar puntos de datos con sus vecinos en una serie de tiempo, lo que resulta en un desenfoque de los bordes n√≠tidos en nuestra serie de tiempo. Por lo tanto, el suavizado de datos es similar al filtrado, en el que se suprimen las altas frecuencias para extraer las se√±ales de baja frecuencia. Con esto, generamos un nuevo punto de datos suavizado para cada punto de datos en la serie temporal.

Para suavizar nuestra serie temporal, podemos elegir entre diferentes enfoques, que difieren principalmente en c√≥mo se calcula el promedio alrededor de un punto de datos. Los enfoques m√°s utilizados son:

- Media m√≥vil simple **(Simple Moving Average)**
- Suavizado por kernel **(Kernel Smoothing)**
- (Simple, Doble, Triple) Suavizado Exponencial (**Simple/Double/Triple Exponential Smoothing)**

El enfoque que elijamos depende de la cantidad de ruido y de los componentes, como la tendencia y la estacionalidad, en nuestra serie temporal.

### **Media m√≥vil simple(Simple Moving Average)**

La media m√≥vil es el enfoque m√°s sencillo que podemos utilizar. Dentro de una ventana deslizante de una longitud fija, todos los puntos de datos se promedian con el mismo peso.

El enfoque es adecuado para datos con poco ruido y para resaltar tendencias a largo plazo. Sin embargo, el promedio m√≥vil no puede capturar la estacionalidad y, por lo tanto, no debe usarse para series de tiempo con estacionalidad y no linealidad compleja. Adem√°s, el enfoque no puede calcular valores al principio y/o al final de la serie temporal, dependiendo de si se calcula una media m√≥vil unilateral o bilateral.

La ventana deslizante se coloca alrededor de un punto de datos y los valores dentro de la ventana se promedian antes de que la ventana se deslice al siguiente punto de datos. Esto se repite hasta que se suaviza la serie temporal. El grado de suavidad est√° determinado por el ancho de la ventana deslizante. Cuanto m√°s ancha sea la ventana, m√°s fuerte ser√° el suavizado.

![Animaci√≥n de una media m√≥vil centrada (de dos caras).](./imagenes/1_S1vibqn_l8Zc4WpV2TqiNw.gif)

Animaci√≥n de una media m√≥vil centrada (de dos caras).

Dependiendo de c√≥mo se coloque la ventana deslizante alrededor del punto de datos, se pueden distinguir dos tipos de medias m√≥viles: unilateral o bilateral. En un enfoque de un solo lado, el punto de datos se coloca al principio o al final de la ventana deslizante, mientras que en un enfoque de dos lados, el punto de datos se coloca en el centro de la ventana deslizante.

La media m√≥vil para un enfoque unilateral se puede calcular mediante

![Untitled](./imagenes/Untitled45.png)

mientras que para un enfoque de dos lados o centrado, la media m√≥vil se puede calcular mediante

![Untitled](./imagenes/Untitled46.png)

en el que *y* es el punto de datos de la serie de tiempo que queremos suavizar, *k* es el ancho de la ventana deslizante y *n* el n√∫mero de pasos de tiempo en la serie de tiempo.

La media m√≥vil se puede implementar f√°cilmente en Python utilizando el m√©todo rolling() de panda. Solo necesitamos pasar el ancho de la ventana deslizante y si queremos usar un enfoque de un solo lado (center=False) o de dos lados (center=True).

```python
smoothed_time_series = time_series.rolling(window=24, center=True).mean()
```

![Ejemplo de c√°lculo del promedio m√≥vil](./imagenes/Untitled47.png)

Ejemplo de c√°lculo del promedio m√≥vil

La aplicaci√≥n de la Media M√≥vil con diferentes anchos de la Ventana Deslizante en una serie de tiempo de carga de electricidad muestra que el suavizado aumenta cuanto m√°s ancha es la Ventana Deslizante. Tambi√©n vemos que cuanto m√°s ancha es la ventana deslizante, m√°s reducimos la longitud de la serie temporal.

![Efecto de suavizado de diferentes anchos de ventana deslizante en una media m√≥vil.](./imagenes/Untitled48.png)

Efecto de suavizado de diferentes anchos de ventana deslizante en una media m√≥vil.

Un ejemplo en Pandas completo podr√≠a ser:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn

# Seaborn para tener estilos de gr√°ficos m√°s bonitos
seaborn.set()

# Cargamos el dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

# Imprimimos las primeras filas para ver los datos
print(df.head())

# Calculamos el promedio m√≥vil con una ventana de 12 meses
df['12_Month_SMA'] = df['Passengers'].rolling(window=12).mean()

# Graficamos los datos originales y el promedio m√≥vil
df[['Passengers', '12_Month_SMA']].plot(figsize=(10,8))
plt.show()
```

Este script carga un conjunto de datos de pasajeros de aerol√≠neas, calcula un promedio m√≥vil con una ventana de 12 meses, y luego traza los datos originales y el promedio m√≥vil. Observar√°s c√≥mo el promedio m√≥vil suaviza las fluctuaciones en los datos y ayuda a resaltar la tendencia a largo plazo. Al visualizar el resultado obtenemos:

![Untitled](./imagenes/Untitled49.png)

### **Suavizado por kernel (Kernel Smoothing)**

El suavizado por kernel es muy similar a la media m√≥vil. La √∫nica diferencia es que usamos diferentes pesos para promediar en la ventana deslizante. Los pesos dependen de la forma de la funci√≥n kernel. Debido a la similitud con la media m√≥vil, las ventajas y desventajas tambi√©n son similares.

![Animaci√≥n de un suavizado de kernel gaussiano.](./imagenes/1_kgd_wvYc5j80HTDDoafcmQ.gif)

Animaci√≥n de un suavizado de kernel gaussiano.

El ancho de la ventana deslizante se puede definir en funci√≥n del ancho completo a la mitad del m√°ximo (FWHM). Con esto, cortamos los extremos posteriores del kernel que tienen valores menores a la mitad de la altura m√°xima de la curva.

A menudo se utiliza un n√∫cleo gaussiano, que representa una distribuci√≥n gaussiana. Con esto, los puntos de datos cercanos al punto de datos en el que queremos suavizar la serie temporal obtienen un mayor peso. La ecuaci√≥n para una distribuci√≥n gaussiana es:

![Untitled](./imagenes/Untitled50.png)

Podemos determinar la desviaci√≥n est√°ndar dependiendo del ancho de nuestra Ventana Deslizante o FWHM por:

![Untitled](./imagenes/Untitled51.png)

El suavizado por kernel se puede implementar f√°cilmente en Python utilizando el m√©todo rolling() de panda. Solo necesitamos definir el kernel que queremos usar como par√°metro win_type. Aqu√≠, podemos elegir entre las funciones de ventana de scipy. Seg√∫n el kernel que elijamos, es posible que debamos pasar par√°metros adicionales, como la desviaci√≥n est√°ndar del kernel gaussiano.

```python
std = fwhm2std(window_size)
smoothed_time_series = time_series.rolling(window=24, win_type="gaussian", center=True).mean(std=std))
```

Al observar los resultados de suavizado de la serie temporal de carga de electricidad, vemos resultados similares a los de la media m√≥vil.

![Efecto de suavizado de diferentes FWHM en un suavizado de kernel gaussiano.](./imagenes/Untitled52.png)

Efecto de suavizado de diferentes FWHM en un suavizado de kernel gaussiano.

Un ejemplo completo de este tipo de suavizado en Python, ser√≠a:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn

def fwhm2std(fwhm):
    return fwhm / np.sqrt(8 * np.log(2))

# Seaborn para tener estilos de gr√°ficos m√°s bonitos
seaborn.set()

# Cargamos el dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

# Definimos el tama√±o de la ventana y la convertimos a desviaci√≥n est√°ndar
window_size = 12
std = fwhm2std(window_size)

# Aplicamos el suavizado Gaussiano con una ventana de 12 meses
df['Gaussian_Smoothed'] = df['Passengers'].rolling(window=window_size, win_type="gaussian", center=True).mean(std=std)

# Graficamos los datos originales y los datos suavizados
df[['Passengers', 'Gaussian_Smoothed']].plot(figsize=(10,8))
plt.show()
```

Y el resultado ser√°:

![Untitled](./imagenes/Untitled53.png)

### **Suavizado exponencial (Exponential Smoothing)**

Como la Media M√≥vil y el Suavizado por Kernel no pueden capturar una no linealidad compleja y no pueden calcular valores al inicio y/o al final de la serie temporal, se desarroll√≥ una alternativa llamada [Suavizado Exponencial](https://es.wikipedia.org/wiki/Suavizamiento_exponencial).

Aqu√≠, todas las observaciones pasadas se ponderan en funci√≥n de una funci√≥n exponencialmente decreciente. Para esto, no se usa una ventana deslizante y los puntos de datos m√°s recientes tienen un mayor peso que los m√°s antiguos.

![Animaci√≥n del Suavizado Exponencial Simple.](./imagenes/1_EcMMPEFaQcW5tdYU1i96CQ.gif)

Animaci√≥n del Suavizado Exponencial Simple.

Al igual que la media m√≥vil y el suavizado de kernel, una parte desafiante del suavizado es elegir los factores de suavizado correctos y los hiperpar√°metros del modelo. Esto se puede hacer manualmente o mediante una optimizaci√≥n.

Podemos elegir entre tres tipos, que difieren en qu√© componentes de la serie temporal pueden manejar: Suavizado exponencial simple, doble y triple. A continuaci√≥n, los tres tipos est√°n ordenados por su complejidad, comenzando por el m√°s simple.

**Suavizado exponencial simple (SES)**
El suavizado exponencial simple, se puede utilizar para series de tiempo estacionarias que no tienen una tendencia o estacionalidad.

El suavizado est√° controlado por un factor de suavizado *Œ±*. El factor de suavizado puede variar entre 0 y 1 y controla la disminuci√≥n exponencial de los pesos. Cuanto mayor sea el factor de suavizado, menor ser√° el peso de los puntos de datos anteriores y, por lo tanto, menor ser√° el suavizado.

El punto de datos suavizado est√° determinado por:

![Untitled](./imagenes/Untitled54.png)

Podemos implementar el suavizado exponencial √∫nico en Python usando el paquete `statsmodel`. Para ello, importamos la clase `SimpleExpSmoothing` desde `statsmodels.tsa.api`.

Pasamos nuestra serie de tiempo a la clase y luego usamos el m√©todo fit() para suavizar la serie de tiempo en funci√≥n de un nivel de suavizado determinado. Para ello, pasamos el *Œ±* a `smoothing_level`. Luego podemos extraer la serie de tiempo suavizada usando `fittedvalues`.

```python
from statsmodels.tsa.api import SimpleExpSmoothing
smoother = SimpleExpSmoothing(time_series)
smoothed_time_series = smoother.fit(smoothing_level=0.5, optimized=False).fittedvalues
```

La clase tambi√©n puede ayudarnos a encontrar un factor de suavizado √≥ptimo *Œ±*. Para esto, no le pasaremos un valor al par√°metro `smoothing_level`. En su lugar, podemos elegir un [m√©todo de optimizaci√≥n](https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.fit.html#statsmodels.tsa.holtwinters.SimpleExpSmoothing.fit).

```python
smoothed_time_series = smoother.fit(method="basinhopping").fittedvalues
```

Comparando diferentes factores de suavizado podemos ver que el suavizado tiene una disminuci√≥n exponencial despu√©s de una fuerte disminuci√≥n de la serie de tiempo original. Cuanto menor sea el factor de suavizado, m√°s lenta ser√° la descomposici√≥n.

![Efecto de suavizado de diferentes factores de suavizado en un suavizado exponencial simple.](./imagenes/Untitled55.png)

Efecto de suavizado de diferentes factores de suavizado en un suavizado exponencial simple.

<aside>
üí° En el suavizado exponencial simple, las observaciones m√°s recientes tienen un mayor impacto en el c√°lculo del promedio. Es √∫til cuando los datos no tienen una tendencia o estacionalidad fuertes.

</aside>

Un ejemplo completo en Python podr√≠a ser:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from statsmodels.tsa.api import SimpleExpSmoothing

# Seaborn para tener estilos de gr√°ficos m√°s bonitos
seaborn.set()

# Cargamos el dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

# Aplicamos el suavizado exponencial simple
model = SimpleExpSmoothing(df['Passengers']).fit(smoothing_level=0.2)
df['SES_Smoothed'] = model.fittedvalues

# Graficamos los datos originales y los datos suavizados
df[['Passengers', 'SES_Smoothed']].plot(figsize=(10,8))
plt.show()
```

Y el resultado ser√°:

![Untitled](./imagenes/Untitled56.png)

**Suavizado exponencial doble (DES)**
El suavizado exponencial doble extiende el suavizado exponencial simple repitiendo el suavizado. Esto permite capturar la tendencia de una serie temporal, pero no se captura la estacionalidad.

Para ello se a√±ade otro factor de suavizado *Œ≤* que controla el suavizado de la tendencia. Similar a *Œ±*, *Œ≤* puede variar entre 0 y 1 y controla la disminuci√≥n exponencial de los pesos.

Como la tendencia puede ser aditiva (es decir, una tendencia lineal) o multiplicativa (es decir, una tendencia exponencial), el suavizado y su implementaci√≥n en Python difieren ligeramente. Por lo tanto, antes de aplicar un suavizado exponencial doble, debemos identificar qu√© tipo de tendencia est√° presente en la serie de tiempo.

En el caso de una tendencia aditiva, el Suavizado Exponencial Doble se puede calcular mediante:

![Untitled](./imagenes/Untitled57.png)

El suavizado exponencial doble para una tendencia aditiva se puede implementar en Python utilizando la clase Holt del paquete `statsmodels`. El uso es similar al suavizado exponencial simple. La √∫nica diferencia es que pasamos un factor de suavizado adicional.

donde l representa el nivel de la serie temporal y r representa la tendencia.

```python
from statsmodels.tsa.api import Holt
smoother = Holt(time_series)
smoothed_time_series = smoother.fit(smoothing_level=0.5, smoothing_trend=0.02, optimized=False).fittedvalues
```

De manera similar al suavizado exponencial √∫nico, tambi√©n podemos usar `statsmodel` para encontrar valores de suavizado √≥ptimos.

Si queremos suavizar una serie de tiempo con una tendencia multiplicativa, necesitamos usar la clase `ExponentialSmoothing`, que se describe en la siguiente secci√≥n cuando discutimos el triple suavizado exponencial.

Aunque no hay tendencia en la serie de tiempo de carga de electricidad, podemos ver el efecto del factor de suavizado *Œ≤*. Si el factor es cero, b√°sicamente usamos un suavizado exponencial simple.

![Efecto de suavizado de diferentes factores de suavizado en un Suavizado Exponencial Doble.](./imagenes/Untitled58.png)

Efecto de suavizado de diferentes factores de suavizado en un Suavizado Exponencial Doble.

Un ejemplo completo en Python podr√≠a ser:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from statsmodels.tsa.api import Holt

# Seaborn para tener estilos de gr√°ficos m√°s bonitos
seaborn.set()

# Cargamos el dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

# Aplicamos el suavizado exponencial doble
model = Holt(df['Passengers']).fit(smoothing_level=0.2, smoothing_trend=0.1)
df['Holt_Smoothed'] = model.fittedvalues

# Graficamos los datos originales y los datos suavizados
df[['Passengers', 'Holt_Smoothed']].plot(figsize=(10,8))
plt.show()
```

Y el resultado ser√°:

![Untitled](./imagenes/Untitled59.png)

<aside>
üí° El Suavizado exponencial doble extiende el suavizado exponencial simple para incluir la tendencia en los datos. Es √∫til cuando los datos tienen una tendencia pero no estacionalidad.

</aside>

**Suavizado exponencial triple (TES)**
El suavizado exponencial triple es el m√©todo de suavizado exponencial m√°s sofisticado. El m√©todo puede capturar la tendencia y la estacionalidad de una serie temporal. El enfoque tambi√©n se denomina Suavizado exponencial de Holt-Winters.

Para esto, se agrega otro factor de suavizado Œ≥ para capturar la estacionalidad. Similar a la tendencia, la estacionalidad puede ser aditiva o multiplicativa.

El suavizado exponencial triple se puede implementar en Python mediante el uso de la clase `ExponentialSmoothing` del paquete `statsmodel`. Aqu√≠, agregamos el par√°metro Œ≥ y tambi√©n podemos indicar si la tendencia o estacionalidad ser√° aditiva o multiplicativa.

```python
from statsmodels.tsa.api import ExponentialSmoothing
smoother = ExponentialSmoothing(time_series)
smoothed_time_series = smoother.fit(smoothing_level=0.5, smoothing_trend=0.02, smoothing_seasonal=0, trend="additive", seasonal="additive", optimized=False).fittedvalues
```

Si establecemos `smoothing_trend`  o `smoothing_seasonal` en cero, obtenemos un suavizado exponencial simple o doble.

Como no existe una tendencia o estacionalidad clara en la serie temporal de carga el√©ctrica, el efecto del factor de suavizado Œ≥ en este caso tambi√©n es muy peque√±o.

![Efecto de suavizado de diferentes factores de suavizado en un suavizado exponencial triple.](./imagenes/Untitled60.png)

Efecto de suavizado de diferentes factores de suavizado en un suavizado exponencial triple.

Un ejemplo completo en Python podr√≠a ser:

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from statsmodels.tsa.api import ExponentialSmoothing

# Seaborn para tener estilos de gr√°ficos m√°s bonitos
seaborn.set()

# Cargamos el dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url, parse_dates=['Month'], index_col='Month')

# Crea el modelo de suavizado exponencial
smoother = ExponentialSmoothing(df['Passengers'], trend='add', seasonal='add', seasonal_periods=12)

# Ajusta el modelo con los par√°metros de suavizado especificados
model = smoother.fit(smoothing_level=0.5, smoothing_trend=0.02, smoothing_seasonal=0.1, optimized=False)

# Agrega la serie suavizada al DataFrame
df['Holt_Winters_Smoothed'] = model.fittedvalues

# Graficamos los datos originales y los datos suavizados
df[['Passengers', 'Holt_Winters_Smoothed']].plot(figsize=(10,8))
plt.show()
```

Y el resultado ser√°:

![Untitled](./imagenes/Untitled61.png)

<aside>
üí° El suavizado exponencial triple extiende a√∫n m√°s el suavizado exponencial para incluir la estacionalidad en los datos. Es √∫til cuando los datos tienen tanto una tendencia como un componente estacional.

</aside>

**Resumen**
Para eliminar ciertas frecuencias o componentes de una serie de tiempo para obtener una vista de la estructura subyacente, podemos elegir entre diferentes enfoques de suavizado.

En los casos donde tenemos tendencia o estacionalidad, debemos elegir un enfoque de suavizado diferente. Adem√°s de elegir el enfoque correcto, tambi√©n debemos tener mucho cuidado al elegir los factores de suavizado. Si los elegimos demasiado peque√±os, podr√≠amos suavizar demasiado la serie temporal y, por lo tanto, suavizar la estructura subyacente. Si elegimos los factores demasiado grandes, es posible que suavicemos la serie temporal y, por lo tanto, no obtengamos una buena visi√≥n de la estructura subyacente.

Sin embargo, implementar los diferentes enfoques es sencillo, por lo que podemos probar f√°cilmente el efecto de diferentes enfoques de suavizado y factores de suavizado para encontrar la combinaci√≥n m√°s adecuada.

## Operaciones con datos de texto (*tokenizaci√≥n*)

La tokenizaci√≥n es una tarea com√∫n con la que se encuentra un cient√≠fico de datos cuando trabaja con datos de texto. Consiste en dividir un texto completo en peque√±as unidades, tambi√©n conocidas como tokens. La mayor√≠a de los proyectos de procesamiento de lenguaje natural (NLP) tienen tokenizaci√≥n como primer paso porque es la base para desarrollar buenos modelos y ayuda a comprender mejor el texto que tenemos.

Aunque la tokenizaci√≥n en Python podr√≠a ser tan simple como escribir .split(), ese m√©todo podr√≠a no ser el m√°s eficiente en algunos proyectos. Por eso, veremos diferentes formas que nos ayudar√°n a tokenizar textos peque√±os, un corpus grande o incluso texto escrito en un idioma diferente al ingl√©s.

### Tokenizaci√≥n simple con .split

Como mencionamos antes, este es el m√©todo m√°s simple para realizar la tokenizaci√≥n en Python. Si escribe .split(), el texto se separar√° en cada espacio en blanco.

Para este y los siguientes ejemplos, utilizaremos un texto narrado por Steve Jobs en el comercial de Apple ‚ÄúThink Different‚Äù.

```python
text = """Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently ‚Äî they‚Äôre not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can‚Äôt do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do."""
text.split()
```

Si escribimos el c√≥digo anterior, obtendremos el siguiente resultado.

```python
['Here‚Äôs', 'to', 'the', 'crazy', 'ones,', 'the', 'misfits,', 'the', 'rebels,', 'the', 
'troublemakers,', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes.', 'The', 
'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they‚Äôre', 'not', 'fond', 'of', 
'rules.', 'You', 'can', 'quote', 'them,', 'disagree', 'with', 'them,', 'glorify', 'or', 
'vilify', 'them,', 'but', 'the', 'only', 'thing', 'you', 'can‚Äôt', 'do', 'is', 'ignore', 
'them', 'because', 'they', 'change', 'things.', 'They', 'push', 'the', 'human', 'race', 
'forward,', 'and', 'while', 'some', 'may', 'see', 'them', 'as', 'the', 'crazy', 'ones,', 
'we', 'see', 'genius,', 'because', 'the', 'ones', 'who', 'are', 'crazy', 'enough', 'to', 
'think', 'that', 'they', 'can', 'change', 'the', 'world,', 'are', 'the', 'ones', 'who', 
'do.']
```

Como podemos ver arriba, el m√©todo split() no considera los s√≠mbolos de puntuaci√≥n como un token separado. Esto podr√≠a cambiar los resultados de su proyecto.

### Tokenizaci√≥n simple con expresiones regulares

La tokenizaci√≥n utilizando expresiones regulares en Python puede realizarse con el m√≥dulo **`re`**. Este m√©todo es altamente flexible ya que podemos definir una expresi√≥n regular propia para dividir el texto. Aqu√≠ vemos un ejemplo simple de c√≥mo hacerlo:

```python
import re

# Definimos el texto
text = "Aqu√≠ tenemos, un ejemplo de texto para tokenizar. ¬°Esto incluye varias oraciones, algunas puntuaciones y palabras!"

# Usamos la funci√≥n findall() de re para encontrar todas las palabras y caracteres de puntuaci√≥n
tokens = re.findall(r'\b\w+\b|[\.\,\!\¬°\¬ø\?]', text)

print(tokens)
```

El resultado ser√°:

```python
['Aqu√≠', 'tenemos', ',', 'un', 'ejemplo', 'de', 'texto', 'para', 'tokenizar', '.', '¬°', 'Esto', 'incluye', 'varias', 'oraciones', ',', 'algunas', 'puntuaciones', 'y', 'palabras', '!']
```

En este caso, la funci√≥n **`re.findall(r'\b\w+\b|[\.\,\!\¬°\?]', text)`** busca todas las secuencias de caracteres alfanum√©ricos delimitadas por bordes de palabra (es decir, las palabras) y los caracteres de puntuaci√≥n especificados (. , ! ¬° ?). El resultado es una lista de palabras y caracteres de puntuaci√≥n como tokens separados.

### **Tokenization con NLTK**

NLTK significa Kit de herramientas de lenguaje natural. Este es un conjunto de bibliotecas y programas para el procesamiento estad√≠stico del lenguaje natural escrito en Python.

NLTK contiene un m√≥dulo llamado¬†`tokenize`¬†con un m√©todo¬†`word_tokenize()`¬†que nos ayudar√° a dividir un texto en tokens. Una vez que haya instalado NLTK, escribir el siguiente c√≥digo para tokenizar el texto.

```python
from nltk.tokenize import word_tokenize
word_tokenize(text)
```

En este caso, la salida predeterminada es ligeramente diferente del m√©todo .split que se muestra arriba.

```python
['Here', '‚Äô', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 
...]
```

En este caso, el ap√≥strofe (') en "Here‚Äôs" y la coma (,) en "ones" se consideraron como tokens.

Para tokenizar en otros idiomas, por ejemplo espa√±ol, podemos hacer:

```python
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

text = "Aqu√≠ est√° una frase en espa√±ol."
tokens = word_tokenize(text, language='spanish')
print(tokens)
```

En este caso, **`word_tokenize`** utilizar√° el modelo de tokenizaci√≥n de Punkt para el espa√±ol.

Otra opci√≥n es usar la biblioteca **`spaCy`**, que proporciona modelos de tokenizaci√≥n espec√≠ficos para varios idiomas, incluyendo ingl√©s, alem√°n, franc√©s, espa√±ol, portugu√©s, italiano, holand√©s y otros.

### Count Vectorizer (sklearn)

Convierta un corpus en un vector con `CountVectorizer` (sklearn): Los m√©todos anteriores se vuelven menos √∫tiles cuando se trata de un corpus grande porque necesitar√° representar los tokens de manera diferente. `CountVectorizer` nos ayudar√° a convertir una colecci√≥n de documentos de texto en un vector de conteo de tokens. Al final, obtendremos una representaci√≥n vectorial de los datos de texto.

Para este ejemplo, agregaremos una cita de Bill Gates al texto anterior para construir un marco de datos que ser√° un ejemplo de un corpus.

```python
import pandas as pd
texts = [
"""Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently ‚Äî they‚Äôre not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can‚Äôt do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.""" ,
 
'I choose a lazy person to do a hard job. Because a lazy person will find an easy way to do it.'
]
df = pd.DataFrame({'author': ['jobs', 'gates'], 'text':texts})
```

Ahora usaremos `CountVectorizer` para transformar estos textos en un vector de conteo de tokens.

```python
from sklearn.feature_extraction.text import CountVectorizer

# Crea un objeto CountVectorizer
vectorizer = CountVectorizer()

# Aprende el vocabulario de los textos y transforma los textos en una matriz de recuentos de tokens
X = vectorizer.fit_transform(df['text'])

# Para ver las caracter√≠sticas (tokens) que se han extra√≠do de los textos, puedes usar vocabulary_
print(vectorizer.vocabulary_)

# Para ver la matriz de recuentos de tokens, puedes convertir X a una matriz dense y luego a un DataFrame
# Usa sorted(vectorizer.vocabulary_) para obtener los tokens ordenados alfab√©ticamente
df_tokens = pd.DataFrame(X.toarray(), columns=sorted(vectorizer.vocabulary_))

df_tokens
```

Si ejecuta ese c√≥digo, obtendr√° un marco que cuenta la cantidad de veces que se menciona una palabra en ambos textos.

![Untitled](./imagenes/Untitled62.png)

Esto se vuelve extremadamente √∫til cuando el marco de datos contiene un gran corpus porque proporciona una matriz con palabras codificadas como valores enteros, que se utilizan como entradas en los algoritmos de aprendizaje autom√°tico.

`CountVectorizer` puede tener diferentes par√°metros como stop_words que definimos anteriormente. Sin embargo, tengamos en cuenta que la expresi√≥n regular predeterminada utilizada por `CountVectorizer` selecciona tokens de 2 o m√°s caracteres alfanum√©ricos (la puntuaci√≥n se ignora por completo y siempre se trata como un separador de token)

### Tokenizaci√≥n de texto en diferentes idiomas con [spaCy](https://spacy.io/)

Cuando necesitemos tokenizar texto escrito en un idioma que no sea ingl√©s, puede usar [spaCy](https://spacy.io/). Esta es una biblioteca para procesamiento avanzado de lenguaje natural, escrita en Python y Cython, que admite tokenizaci√≥n para m√°s de 65 idiomas.

Vamos a tokenizar el mismo texto de Steve Jobs pero ahora traducido al espa√±ol.

```python
from spacy.lang.es import Spanish
nlp = Spanish()

text_spanish = """Por los locos. Los marginados. Los rebeldes. Los problematicos. 
Los inadaptados. Los que ven las cosas de una manera distinta. A los que no les gustan
las reglas. Y a los que no respetan el ‚Äústatus quo‚Äù. Puedes citarlos, discrepar de ellos,
ensalzarlos o vilipendiarlos. Pero lo que no puedes hacer es ignorarlos‚Ä¶ Porque ellos
cambian las cosas, empujan hacia adelante la raza humana y, aunque algunos puedan
considerarlos locos, nosotros vemos en ellos a genios. Porque las personas que est√°n
lo bastante locas como para creer que pueden cambiar el mundo, son las que lo logran."""

doc = nlp(text_spanish)

tokens = [token.text for token in doc]

print(tokens)
```

En este caso, importamos espa√±ol de [`spacy.lang.es`](http://spacy.lang.es/) pero si est√° trabajando con texto en ingl√©s, simplemente importe ingl√©s de `spacy.lang.en`. Se puede consultar la lista de idiomas disponibles [aqu√≠](https://spacy.io/usage/models). Si ejecutamos este c√≥digo, obtendremos el siguiente resultado:

```python
['Por', 'los', 'locos', '.', 'Los', 'marginados', '.', 'Los', 'rebeldes', '.', 'Los', 'problematicos', '.', '\n', 'Los', 'inadaptados', '.', 'Los', 'que', 'ven', 'las', 'cosas', 'de', 'una', 'manera', 'distinta', '.', 'A', 'los', 'que', 'no', 'les', 'gustan', '\n', 'las', 'reglas', '.', 'Y', 'a', 'los', 'que', 'no', 'respetan', 'el', '‚Äú', 'status', 'quo', '‚Äù', '.', 'Puedes', 'citarlos', ',', 'discrepar', 'de', 'ellos', ',', '\n', 'ensalzarlos', 'o', 'vilipendiarlos', '.', 'Pero', 'lo', 'que', 'no', 'puedes', 'hacer', 'es', 'ignorarlos', '‚Ä¶', 'Porque', 'ellos', '\n', 'cambian', 'las', 'cosas', ',', 'empujan', 'hacia', 'adelante', 'la', 'raza', 'humana', 'y', ',', 'aunque', 'algunos', 'puedan', '\n', 'considerarlos', 'locos', ',', 'nosotros', 'vemos', 'en', 'ellos', 'a', 'genios', '.', 'Porque', 'las', 'personas', 'que', 'est√°n', '\n', 'lo', 'bastante', 'locas', 'como', 'para', 'creer', 'que', 'pueden', 'cambiar', 'el', 'mundo', ',', 'son', 'las', 'que', 'lo', 'logran', '.']
```

Como podemos ver, spaCy considera los s√≠mbolos de puntuaci√≥n como un token separado (incluso se incluyeron las nuevas l√≠neas\n).

Pero, ¬øpor qu√© necesitamos un tokenizador para cada idioma?

Aunque para idiomas como el espa√±ol y el ingl√©s, la tokenizaci√≥n ser√° tan simple como separar por espacios en blanco, para otros idiomas como el chino y el japon√©s, la ortograf√≠a podr√≠a no tener espacios para delimitar "palabras" o "tokens". En tales casos, una biblioteca como spaCy ser√° √∫til. [Aqu√≠ puede consultar m√°s sobre la importancia de la tokenizaci√≥n en diferentes idiomas.](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language)

### Tokenizaci√≥n con Gensim

Gensim es una biblioteca para modelado de temas no supervisados y procesamiento de lenguaje natural y tambi√©n contiene un tokenizador. Una vez que instalemos Gensim, tokenizar el texto ser√° tan simple como escribir el siguiente c√≥digo.

```python
from gensim.utils import tokenize
list(tokenize(text))
```

El resultado de este c√≥digo es este:

```python
['Here', 's', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'The', 'ones', 'who', 'see', 'things', 'differently', 'they', 're', 'not', 'fond', 'of', 'rules', 'You', 'can', 'quote', 'them', 'disagree', 'with', 'them', 'glorify', 'or', 'vilify', 'them', 'but', 'the', 'only', 'thing', 'you', 'can', 't', 'do', 'is', 'ignore', 'them', 'because', 'they', 'change', 'things', 'They', 'push', 'the', 'human', 'race', 'forward', 'and', 'while', 'some', 'may', 'see', 'them', 'as', 'the', 'crazy', 'ones', 'we', 'see', 'genius', 'because', 'the', 'ones', 'who', 'are', 'crazy', 'enough', 'to', 'think', 'that', 'they', 'can', 'change', 'the', 'world', 'are', 'the', 'ones', 'who', 'do']
```

Como se puede ver, Gensim separa cada vez que encuentra con un s√≠mbolo de puntuaci√≥n, por ejemplo: Here, s, can, t

### Tokenizaci√≥n de oraciones (Sentence Tokenization)

La tokenizaci√≥n de oraciones, como la que se realiza con **`sent_tokenize`** de NLTK, se utiliza a menudo como un paso de preprocesamiento en diversas tareas de Procesamiento del Lenguaje Natural (NLP). Algunas aplicaciones incluyen:

- **An√°lisis de sentimientos**: En este contexto, podr√≠as querer dividir un texto en oraciones para analizar los sentimientos de cada oraci√≥n por separado. Por ejemplo, en una cr√≠tica de pel√≠cula, una oraci√≥n puede expresar un sentimiento positivo ("La actuaci√≥n fue excelente") y la siguiente puede expresar un sentimiento negativo ("La trama era predecible").
- **Traducci√≥n autom√°tica**: Los sistemas de traducci√≥n autom√°tica a menudo traducen textos oraci√≥n por oraci√≥n, ya que las oraciones suelen ser unidades de significado independientes.
- **Resumen autom√°tico**: Algunos algoritmos de resumen autom√°tico de textos operan a nivel de oraciones. Por ejemplo, el algoritmo puede seleccionar un subconjunto de oraciones del texto original que cubran la mayor parte del contenido.
- **Extracci√≥n de informaci√≥n**: La extracci√≥n de entidades nombradas, las relaciones entre entidades y los eventos a menudo se realiza a nivel de oraciones.
- **L√≠mite de tokens en los LLM**: Los grandes modelos de lenguaje (LLM) como BERT, GPT-3, GPT-4, etc., tienen un l√≠mite en la longitud m√°xima de tokens que aceptan como prompt de entrada. Si tenemos un texto que es m√°s largo que este l√≠mite, necesitaremos dividirlo en pedazos m√°s peque√±os para poder pasar cada pedazo (chunk) al modelo.

Aqu√≠ vemos un ejemplo de uso de `sent_tokenize`:

```python
import requests
from nltk.tokenize import sent_tokenize
import nltk

# Se requiere el paquete punkt para que funcione el ejemplo
nltk.download('punkt')

# Descarga el archivo de texto
url = 'https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt'
response = requests.get(url)
text = response.text

# Tokeniza el texto en oraciones
sent_tokens = sent_tokenize(text)

# Imprime los tokens
print("Sentence Tokens:", sent_tokens[:5])  # Imprime las primeras 5 oraciones tokenizadas
```

Y el resultado es:

```
Sentence Tokens: ['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.', 'Members of Congress and the Cabinet.', 'Justices of the Supreme Court.', 'My fellow Americans.', 'Last year COVID-19 kept us apart.']
```