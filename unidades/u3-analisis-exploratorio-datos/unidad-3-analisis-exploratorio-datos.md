# Unidad 3 - An√°lisis explcorrelationoratorio de datos: Medidas de Resumen

### Fundamentos de ciencia de datos

![Banner FCD-1.png](./imagenes/Banner_FCD-1.png)

## Introducci√≥n

Una de las primeras tareas que tenemos que realizar antes de comenzar a trabajar con los datos que recibimos es el an√°lisis exploratorio de los datos. Esta tarea surge naturalmente cuando la consigna del an√°lisis no est√° definida pero es muy posible pasarla por alto cuando nos piden realizar X an√°lisis usando ciertos datos. 

En esta unidad vamos a estudiar medidas de resumen que nos permitan entender los datos a partir de ciertos valores. En la pr√≥xima unidad, vamos a explorar los datos a trav√©s de visualizaciones.

Para esta unidad vamos a utilizar la ‚ÄúEncuesta Nacional de Factores de Riesgo (ENFR) 2018‚Äù disponible [ac√°](https://www.indec.gob.ar/indec/web/Institucional-Indec-BasesDeDatos-2) 

### Contenido de la unidad

1. Medidas de centralidad: media, mediana, moda
2. Cuartiles y percentiles
3. Medidas de dispersi√≥n: rango, varianza, desv√≠o est√°ndar, rango intercuart√≠lico, desviaci√≥n mediana absoluta (MAD)
4. Valores at√≠picos
5. Tablas de frecuencia, proporciones y porcentajes
6. M√©tricas de correlaci√≥n: covarianza, correlaci√≥n lineal, correlaci√≥n de Spearman
7. Matriz de covarianza y matriz de correlaci√≥n
8. M√©tricas de distancia y similaridad

## Algunos conceptos importantes

### **Poblaci√≥n**

En estad√≠stica, la poblaci√≥n se refiere al conjunto total de individuos, objetos, eventos o medidas que comparten una caracter√≠stica com√∫n de inter√©s y que son de inter√©s para un estudio en particular. La poblaci√≥n puede ser tan grande o tan peque√±a como se desee y puede estar formada por personas, animales, objetos, empresas, pa√≠ses, entre otros.

### **Muestra**

Por otro lado, una muestra se refiere a una porci√≥n o subconjunto seleccionado que se extrae de la poblaci√≥n con el prop√≥sito de realizar inferencias sobre la poblaci√≥n en su conjunto. El procedimiento por el cual se selecciona una muestra de una poblaci√≥n determinada se denomina ***muestreo***. Existen diversos tipos de muestreo que, en t√©rminos generales, se clasifican en funci√≥n de si interviene o no el azar en el proceso. Cabe destacar que para garantizar que los resultados obtenidos a partir de la muestra sean generalizables a la poblaci√≥n en su conjunto, es requisito que la muestra sea seleccionada de manera aleatoria.

Por ejemplo, si se desea estudiar la estatura de las mujeres adultas en un pa√≠s en particular, se podr√≠a seleccionar una muestra aleatoria de mujeres adultas y medir su estatura. Si la muestra es representativa y suficientemente grande, se puede inferir informaci√≥n sobre la estatura promedio de todas las mujeres adultas en ese pa√≠s.

![Untitled](./imagenes/Untitled.png)

### **Variable**

Una variable es una caracter√≠stica, cualidad o propiedad observada que puede asumir diferentes valores y es susceptible de ser cuantificada o medida en una investigaci√≥n. Las observaciones registradas de una o m√°s variables conforman un **conjunto o set de datos**. Seg√∫n su naturaleza, las variables pueden ser:

- Cualitativas (tambi√©n llamadas categ√≥ricas): son aquellas que no son medibles num√©ricamente.
- Cuantitativas: son aquellas que toman valores num√©ricos para los cuales tiene sentido pensar en operaciones aritm√©ticas. Pueden ser continuas, si pueden asumir te√≥ricamente cualquier valor dentro de un intervalo, o discretas, si s√≥lo pueden tomar valores aislados en dicho intervalo.

En el ejemplo anterior, la variable de inter√©s es ‚Äúestatura de las mujeres adultas de un pa√≠s‚Äù y se trata de una variable cuantitativa continua.

![Untitled](./imagenes/Untitled%201.png)

### Medici√≥n

Asignaci√≥n de n√∫meros a objetos, personas o hechos, para representar cantidades de sus atributos, de acuerdo a ciertas reglas.

Escala de medida: 

- Nominal: los n√∫meros son simplemente r√≥tulos, no tienen valores cuantitativos y las categor√≠as son mutuamente excluyentes. Ej: Profesiones.
- Ordinal: los n√∫meros indican un orden en la medici√≥n, pero no brinda informaci√≥n sobre las
distancias entre cada nivel. Ej: orden de llegada en una carrera, grado de dolor.
- Intervalo: las diferencias entre las mediciones indican intervalos que tienen sentido. Ausencia del cero absoluto. Ej: temperatura en ¬∞C, puntajes de cr√©dito. Esto significa que no puedes decir que 20¬∞ centigrados es el doble de 10¬∞ grados centigrados.
- Raz√≥n: Existe un cero absoluto  y las razones tienen sentido. Ej: estatura.

La siguiente tabla muestra las transformaciones y estad√≠sticas aplicables a cada tipo:

![Untitled](./imagenes/Untitled%202.png)

## Medidas de centralidad

Las medidas de centralidad describen el centro de la distribuci√≥n de los datos. Las m√°s comunes son la media aritm√©tica, las fractilas o cuantilos y el modo.

<aside>
üí° Algunas medidas de resumen se calculan de forma diferente dependiendo si el c√°lculo se realiza con la poblaci√≥n o con una muestra.

</aside>

### **Media aritm√©tica/Promedio**

El promedio de la poblaci√≥n se representa con la letra $\mu$ mientras que el promedio de la muestra se representa con $\bar{x}$. En este √∫ltimo caso, se calcula sumando todos los valores no nulos de la variable $X$ y dividiendo por la cantidad de valores no nulos de $X$ ($n$):

$$
\bar{x} = \frac{\sum_{i = 1}^{n}{x_{i}}}{n}
$$

Una cuesti√≥n importante a tener en cuenta sobre esta medida es que, como depende de todos los valores registrados de la variable, la presencia de un valor anormalmente grande o peque√±o en el conjunto influye profundamente sobre ella. Por lo tanto, en estas situaciones deja de ser una medida de centralidad representativa del conjunto de observaciones y deber√° buscarse otra alternativa m√°s adecuada. 

Para calcularla, contamos con el m√©todo¬†**`mean()`**¬†de¬†**Pandas**, que ya empleamos anteriormente.

### **Mediana**

La mediana, segundo cuartil o percentil del 50%, se define como el valor de la variable que se encuentra en la posici√≥n central del conjunto de datos ordenado de menor a mayor. En los casos en los que no existe un √∫nico valor central, es decir cuando la cantidad de valores estudiados es par, se calcula el promedio de los dos valores centrales. 

Veamos un ejemplo:

1. Tenemos el siguiente conjunto de 15 observaciones de la variable $X$: [40, 2, 2, 3, 9, 10, 11, 11, 1, 3, 10, 5, 1, 3, 3]
2. Ordenamos los valores de la variable [1, 1, 2, 2, 3, 3, 3, 3, 5, 9, 10, 10, 11, 11, 40]
3. El valor intermedio es 3, ya que es el que deja el mismo n√∫mero de observaciones (7) por debajo y por encima de √©l.

En nuestros datos parece haber un valor at√≠pico, el 40. Como discutimos arriba, la media, 7.6, se ve afectada por este valor y no constituye una medida de centralidad representativa del conjunto. En esta situaci√≥n, la mediana es una medida de tendencia central m√°s adecuada para informar.

### **Fractilas o cuantilos**

La mediana es un tipo particular de medida que pertenece a una clase mucho m√°s general de estad√≠sticas descriptivas de centralidad: **las fractilas o cuantilos**. En t√©rminos generales, la fractila de orden r es aquel valor de la variable tal que el r% de las observaciones del conjunto ordenado de datos son menores o iguales a √©l.

**Percentiles**

Los percentiles son un tipo de fractila que divide al conjunto de datos ordenados de la variable $X$ en 100 partes con aproximadamente el mismo n√∫mero de datos.

Supongamos que tenemos la siguiente lista ordenada de datos con 11 elementos:

$$
l = [1,2,3,4,5,6,7,8,9,10,11]
$$

Una vez ordenados los datos de una variable $X$ de menor a mayor, el percentil de orden $i$ es el valor de la variable que deja al $i\%$ de los datos por debajo de s√≠ y al $100 - i\%$ por encima de s√≠.  El percentil 10 es **2**, puesto que deja un √∫nico valor por debajo de s√≠ y nueve valores por encima.

**Los cuartiles**

Los cuartiles son un tipo de fractila que divide al conjunto de datos ordenados de la variable en 4 partes con aproximadamente el mismo n√∫mero de datos:

- **Cuartil 1 (Q1)** es el valor tal que el 25% de los datos son menores o iguales a √©l. Coincide con el percentil 25.
- **Cuartil 2 (Q2)** es el valor tal que el 50% de los datos son menores o iguales a √©l. Coincide con la mediana, definida anteriormente, y con el percentil 50.
- **Cuartil 3 (Q3)** es el valor tal que el 75% de los datos son menores o iguales a √©l. Coincide con el percentil 75.

**¬øC√≥mo calculamos estas medidas con NumPy/Pandas?**

**Pandas** posee el m√©todo `quantile` , que permite calcular cualquier fractila de un conjunto de datos de una variable. La misma puede especificarse en el par√°metro **`q`** , que por *default* es igual a 0.5 

```python
data.bhih01.quantile(0.1)
6000
```

Tambi√©n puede utilizarse `percentile`, de **NumPy**, con la salvedad de que, a diferencia del primer m√©todo, debe especificarse el percentilo a calcular como un n√∫mero entero entre 1 y 100.

```jsx
np.percentile(data.bhih01, q = 10)
6000
```

Existen varios m√©todos para calcular percentiles. El **m√©todo R-7**, tambi√©n conocido como "R-7 de Hyndman & Fan", es una variante del m√©todo de interpolaci√≥n lineal. En el mismo, la posici√≥n de un percentil espec√≠fico en una serie de datos ordenada se calcula utilizando la siguiente f√≥rmula, donde **`percentil`** es el percentil deseado y **`N`** es el n√∫mero total de elementos en la serie de datos:

```python
# percentil puede ser cualquier valor entre 1 y 100
pos = 1 + (percentil / 100) * (N - 1)
```

<aside>
‚ö†Ô∏è **Atenci√≥n:**¬†esta f√≥rmula nos indica la posici√≥n del percentil y no su valor, ya que no estamos incluyendo ninguna de las observaciones que componen el conjunto de datos.

</aside>

Cuando **`pos`** no es un entero, se **interpola linealmente** entre los dos valores adyacentes en la posici√≥n fraccionaria en la serie de datos ordenada. El **m√©todo R-7** es utilizado por *default* tanto por `NumPy` [](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)como por `Pandas` para calcular percentiles. 

[Los otros m√©todos R (R-1 a R-9)](https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample) son una familia de m√©todos de estimaci√≥n de percentiles propuestos por Hyndman y Fan en su art√≠culo "[Sample Quantiles in Statistical Packages](https://www.amherst.edu/media/view/129116/original/Sample+Quantiles.pdf)" (1996). Estos m√©todos difieren en la forma en que se calcula la posici√≥n y en c√≥mo se interpola entre valores adyacentes cuando la posici√≥n no es un entero. Es importante destacar que no existe un enfoque "correcto" √∫nico para calcular percentiles, ya que cada m√©todo tiene sus propias ventajas y desventajas en funci√≥n de la distribuci√≥n y caracter√≠sticas de los datos en cuesti√≥n. Por lo tanto, es √∫til conocer los diferentes m√©todos y seleccionar el que mejor se adapte a las necesidades del an√°lisis de datos.

En el caso de **Pandas**, el m√©todo `quantile()` cuenta con un par√°metro llamado `interpolation`, que permite definir el m√©todo a utilizar para estimar la fractila. Si bien utiliza `interpolation = 'linear'` por *default* (M√©todo R-7), permite elegir entre otros criterios.

**Ejemplos:**

La siguiente lista de longitud 8 est√° dividida en 4 partes iguales y pintada acorde  [**2, 2, 4, 5, 5, 6, 7, 8**]. Veamos c√≥mo se calcular√≠an los tres cuartilos utilizando el m√©todo R-7.

El Q1 es un valor que se ubica entre el √≠ndice 2 y el 3 (Pos. √≠ndice 2.75) para que la lista quede dividida en dos partes, por un lado el 25% de los datos y por el otro el 75%. La lista deber√≠a quedar de la siguiente manera [**2, 2, 4, 5, 5, 6, 7, 8**]. El Q1 en este caso es 3.5 si utilizamos la interpolaci√≥n lineal para calcular el valor. 

Posici√≥n del cuartil Q1 = (1 + 0.25 * (8 - 1))  = 2.75

Valor del cuartil Q1 = 2 + 0.75 * (4 - 2) = 3.5

El Q2 es un valor que se ubica entre el √≠ndice 4 y 5 (Pos. √≠ndice 4.5). Como ambos √≠ndices tienen el mismo valor, 5, no hay dudas de que el Q2 es igual a 5. La lista quedar√≠a dividida de esta manera [**2, 2, 4, 5, 5, 6, 7, 8**] con 50% de los datos por debajo del valor y 50% de los datos por encima. 

Posici√≥n del cuartil Q2 = (1 + 0.5 * (8 - 1))  = 4.5

Valor del cuartil Q2 = 5 + 0.5 * (5 - 5) = 5

Finalmente, el Q3, es un valor que se ubica entre el √≠ndice 6 y 7 (Pos. √≠ndice 6.25) para que la lista quede dividida en 75-25%, [**2, 2, 4, 5, 5, 6, 7, 8**] . En este caso el Q3 es 6.25.

Posici√≥n del cuartil Q3 = (1 + 0.75 * (8 - 1))  = 6.25

Valor del cuartil Q3 = 6 + 0.25 * (7 - 6) = 6.25

En el siguiente ejemplo, mostramos c√≥mo calcular los cuartiles con **Pandas** recurriendo al m√©todo [`quantile`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html), pero tambi√©n desarrollando las f√≥rmulas en Python ‚Äúa mano‚Äù.

```python
import pandas as pd
import math
serie = pd.Series([2, 2, 4, 5, 5, 6, 7, 8])

#------------------ "A MANO" ------------------
# C√°lculo de la posici√≥n seg√∫n m√©todo R7 (Mismo que en Excel, R, Pandas, NumPy)
q1_pos = 1 + (25 / 100) * (serie.size - 1)
q2_pos = 1 + (50 / 100) * (serie.size - 1)
q3_pos = 1 + (75 / 100) * (serie.size - 1)

# Obtenemos la parte decimal de la posici√≥n calculada
parte_frac, _ = math.modf(q1_pos)
# ¬°Restamos 1 a la posici√≥n, ya que nuestra serie se basa en 0!
index = int(q1_pos) - 1
# Valor del cuartil Q1 = 2 + 0.75 * (4 - 2) = 3.5
q1_valor = serie[index] + parte_frac * (serie[index + 1] - serie[index])

# Obtenemos la parte decimal de la posici√≥n calculada
parte_frac, _ = math.modf(q2_pos)
# ¬°Restamos 1 a la posici√≥n, ya que nuestra serie se basa en 0!
index = int(q2_pos) - 1
# Valor del cuartil Q2 = 5 + 0.5 * (5 - 5) = 5
q2_valor = serie[index] + parte_frac * (serie[index + 1] - serie[index])

# Obtenemos la parte decimal de la posici√≥n calculada
parte_frac, _ = math.modf(q3_pos)
# ¬°Restamos 1 a la posici√≥n, ya que nuestra serie se basa en 0!
index = int(q3_pos) - 1
# Valor del cuartil Q3 = 6 + 0.25 * (7 - 6) = 6.25
q3_valor = serie[index] + parte_frac * (serie[index + 1] - serie[index])

#------------------ USANDO PANDAS ------------------
# Ahora usamos pandas para calcular los cuartiles. 
# Por defecto Pandas usa siempre la interpolaci√≥n "linear"
q1_pandas = serie.quantile(0.25, interpolation="linear")
q2_pandas = serie.quantile(0.50, interpolation="linear")
q3_pandas = serie.quantile(0.75, interpolation="linear")

# Los valores calculados a mano en Python, son iguales a Pandas
print('Posic.', '\t\t', 'Python', '\t', 'Pandas')
print(q1_pos, '\t\t', q1_valor, '\t\t', q1_pandas)
print(q2_pos, '\t\t', q2_valor, '\t\t', q2_pandas)
print(q3_pos, '\t\t', q3_valor, '\t\t', q3_pandas)

# Resultado
"""
Posic. 	 Python  Pandas
2.75 		 3.5 		 3.5
4.5 		 5.0 		 5.0
6.25 		 6.25    6.25
"""
```

Volviendo a los datos de ingreso reportados en la encuesta de nuestro dataset, podemos realizar los siguientes c√°lculos.

```python
# Cuartil 1 (Q1)
data.bhih01.quantile(0.25)
10000

# Cuartil 2 (Q2)
data.bhih01.quantile(0.5)
18000

# Cuartil 3 (Q3)
data.bhih01.quantile(0.75)
30000
```

<aside>
ü§î

**Para pensar‚Ä¶**
¬øC√≥mo interpretar√≠a cada una de estas medidas calculadas sobre los datos de la variable `bhih01` si tuviera que informar estos resultados?

</aside>

### **Moda**

Es el valor de la variable que se presenta un mayor n√∫mero de veces, es decir, el que tiene la mayor frecuencia. Puede ocurrir que un conjunto de datos no presente modo (si todos los valores presentan igual frecuencia) o que haya m√°s de uno.

El comportamiento de una variable a nivel poblacional se encuentra descripto por su **distribuci√≥n de probabilidad**, la cual indica el rango de valores que √©sta puede asumir junto con sus respectivas probabilidades. El concepto de moda puede trasladarse a esta situaci√≥n y corresponde a aquel valor de la variable en el que la funci√≥n densidad de probabilidad (en el caso de que la variable sea continua) alcanza un m√°ximo. En funci√≥n del n√∫mero de modas que pose, una distribuci√≥n de probabilidad puede ser unimodal, bimodal o multimodal:  

![Untitled](./imagenes/Untitled%203.png)

Libro ‚ÄúHands-On Data Analysis with Pandas‚Äù Stefanie Molin.

**Pandas** cuenta con funciones espec√≠ficas para calcular estas medidas:

```python
import pandas as pd
data = pd.read_csv('ENFR 2018 - Base usuario.txt', delimiter = '|')

# Mean
data.bhih01.mean()
2446.65

# Median
data.bhih01.median()
18000

# Mode
data.bhih01.mode()
20000
```

Algo importante para agregar es que la moda es la √∫nica medida de posici√≥n que puede usarse para datos provenientes de una variable cualitativa. Si se aplica el m√©todo `mode()` sobre una columna del **DataFrame** que contiene observaciones para este tipo de variable, devolver√° aquella/s categor√≠a/s que se hayan presentado con mayor frecuencia.

## Medidas de dispersi√≥n

Las medidas de dispersi√≥n describen la variabilidad de los datos y complementan a las medidas de centralidad, que no resumen en forma completa la informaci√≥n contenida en el conjunto de datos de la variable de inter√©s. Podemos tener dos conjuntos de datos con aproximadamente la misma media, mediana y modo, pero que difieran en cu√°nto se alejan del valor ‚Äúcentral‚Äù.

### Rango

Rango o amplitud, es la distancia entre el dato de menor valor y el de mayor valor. Dado un conjunto de datos de una variable llamado $valores$, el rango se define como:

$$
R = max(valores)- min(valores)
$$

Por ejemplo, para el grupo de datos [5, 7, 29, 9, 60, 1, 100, 45, 50, 80], el rango es 99.

Usando los datos de la encuesta de salud obtendr√≠amos:

```python
data.bhih01.max() - data.bhih01.min()
420000
```

### Varianza

Es una medida de cu√°nto se desv√≠an, en promedio, las observaciones de una variable respecto a la media aritm√©tica. Se calcula como:

$$
s^2=\frac {\sum_{i = 1}^{n}(x_i - \bar{x})^2}{n-1}
$$

Donde:

$n:$ Cantidad de datos en la muestra

$\bar{x}:$ Media de la muestra

<aside>
üí° La varianza se utiliza en diversos campos como la econom√≠a, finanzas, psicolog√≠a, biolog√≠a, f√≠sica, entre otros, para evaluar qu√© tan dispersos est√°n los valores en un conjunto de datos. Si la varianza es peque√±a, significa que los datos est√°n m√°s agrupados o concentrados en torno a la media; si es grande, los datos est√°n m√°s dispersos.

</aside>

### Desv√≠o Est√°ndar

Se define como la ra√≠z cuadrada positiva de la variancia:

$$
s=\sqrt{\frac {\sum_{i = 1}^{n}(x_i - \bar{x})^2}{n-1}}
$$

En comparaci√≥n con la variancia, posee la ventaja de que est√° expresada en las unidades de la variable, por lo que su interpretaci√≥n es m√°s sencilla y directa. Podemos interpretarla como una¬†**‚Äúdistancia promedio de las observaciones con respecto a la media‚Äù**.

Una desviaci√≥n est√°ndar baja indica que la mayor parte de los datos de una muestra tienden a estar agrupados cerca de la media aritm√©tica, mientras que una desviaci√≥n est√°ndar alta indica que los datos se extienden sobre un rango de valores m√°s amplio. 

Gr√°ficamente, considerando que las representaciones de la figura corresponden a gr√°ficos de densidad de los datos observados de una variable (ya los trabajaremos en profundidad en la pr√≥xima Unidad):

![Una desviaci√≥n est√°ndar alta indica una mayor variabilidad en los datos, mientras que una desviaci√≥n est√°ndar baja indica una menor variabilidad.](./imagenes/Untitled%204.png)

Una desviaci√≥n est√°ndar alta indica una mayor variabilidad en los datos, mientras que una desviaci√≥n est√°ndar baja indica una menor variabilidad.

Pensando en la distribuci√≥n de probabilidad de la variable, si √©sta sigue una distribuci√≥n normal:

![Una gr√°fica de la¬†[distribuci√≥n normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal)¬†(o curva en forma de campana, o curva de Gauss), donde cada banda tiene un ancho de una vez la desviaci√≥n est√°ndar](./imagenes/Untitled%205.png)

Una gr√°fica de la¬†[distribuci√≥n normal](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_normal)¬†(o curva en forma de campana, o curva de Gauss), donde cada banda tiene un ancho de una vez la desviaci√≥n est√°ndar

**Pandas** cuenta con la funci√≥n `pd.std`que calcula la desviaci√≥n est√°ndar de la muestra:

```python
import pandas as pd
data = pd.read_csv('ENFR 2018 - Base usuario.txt', delimiter = '|')

# Desviaci√≥n est√°ndar
data.bhih01.std()
19756.58
```

### Coeficiente de Variaci√≥n (CV)

El coeficiente de variaci√≥n (CV) es una medida de dispersi√≥n relativa que se utiliza para comparar la variabilidad de diferentes conjuntos de datos en t√©rminos de su relaci√≥n con la media. Se define como el desv√≠o est√°ndar dividido por la media aritm√©tica:

$$
CV = \frac{s}{\bar{|x|}}
$$

Es una medida adimensional que indica qu√© proporci√≥n representa el desv√≠o est√°ndar de la media aritm√©tica. Se suele utilizar cuando se desea comparar la variabilidad de dos o m√°s conjuntos de datos que difieren en unidades de medida y/o magnitudes.

Por ejemplo, si queremos comparar la variabilidad de los precios de dos productos que tienen diferentes unidades de medida, el CV nos permitir√≠a hacerlo de manera m√°s adecuada. Si el producto A tiene un precio promedio de $100 con una desviaci√≥n est√°ndar de $10, y el producto B tiene un precio promedio de $50 con una desviaci√≥n est√°ndar de $5, el CV de ambos productos ser√≠a:

CV A = (10 / 100) x 100% = 10%
CV B = (5 / 50) x 100% = 10%

En este caso, el CV es el mismo para ambos productos, lo que indica que la variabilidad relativa del precio es similar en ambos casos, independientemente de las unidades de medida utilizadas.

### Rango intercuart√≠lico (RI)

Es la diferencia entre el tercer y el primer cuartil y, como tal, **mide la dispersi√≥n del 50% de los datos centrales.**

$$
RI = Q_3 - Q_1
$$

El rango intercuart√≠lico es una medida de dispersi√≥n resistente a la presencia de valores at√≠picos o extremos que pueden distorsionar otras medidas de dispersi√≥n, como la desviaci√≥n est√°ndar. Por lo tanto, cuando se usa la mediana como medida de centralidad, el RI es una medida de dispersi√≥n adecuada para acompa√±arla. 

Como veremos debajo, corresponde al ancho de la caja que constituye una parte central del gr√°fico conocido como **boxplot**.

![Untitled](./imagenes/Untitled%206.png)

### Desviaci√≥n Mediana Absoluta (MAD)

La desviaci√≥n mediana absoluta se define como la mediana de la desviaci√≥n absoluta de cada punto con respecto a la mediana:

$$
MAD = Mediana(|X_{i} - Q_{2}|)
$$

Supongamos que tenemos el siguiente conjunto de valores observados: [1,2,3,4,5,6,7,8,9,10,11]. Para calcular la MAD seguimos los siguientes pasos:

1. Calcular la mediana: 6
2. Calcular la desviaci√≥n absoluta de cada observaci√≥n con respecto a la mediana: [5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5]
3. Ordenar la lista de desviaciones absolutas: [0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5] 
4. Calcular mediana de las desviaciones absolutas: 3

La MAD se utiliza para estimar la variabilidad de los datos en diferentes campos, como la estad√≠stica, la ciencia de datos, entre otros. A diferencia de la desviaci√≥n est√°ndar, la MAD es una medida que no se ve afectada por los valores extremos o at√≠picos que pueden influir en otras medidas de dispersi√≥n.

La MAD se utiliza a menudo en conjunci√≥n con la mediana, para proporcionar una descripci√≥n completa de la tendencia central y la variabilidad de los datos. Por ejemplo, si queremos describir la distribuci√≥n de un conjunto de datos que contiene valores at√≠picos o extremos, la mediana y la MAD pueden proporcionar medidas m√°s representativas de la tendencia central y la variabilidad de los datos que la media y la desviaci√≥n est√°ndar.

### **Diagrama de Caja y Bigotes (Boxplot)**

Los cuartilos y los valores m√≠nimo y m√°ximo conforman un conjunto de cinco n√∫meros que brindan un buen resumen de nuestros datos y con los cuales podemos construir un gr√°fico llamado boxplot. Estos gr√°ficos, tambi√©n conocidos como diagramas de caja y bigotes, se utilizan para representar gr√°ficamente la distribuci√≥n de un conjunto de datos num√©ricos y mostrar la presencia de potenciales valores at√≠picos (**outliers**).

Se compone de una caja, que abarca todo el rango de valores entre el primer y el tercer cuartil, y de dos l√≠neas (*bigotes*), que se extienden a partir de los bordes de la caja hasta los valores m√≠nimo y m√°ximo del conjunto de datos. Adicionalmente, una l√≠nea vertical en el interior de la caja representa la mediana.

![boxplot.png](./imagenes/boxplot.png)

### El boxplot modificado

Como se mencion√≥ anteriormente, existe una versi√≥n modificada del boxplot permite detectar potenciales ***outliers***, es decir, observaciones que no son t√≠picas del conjunto.

El criterio com√∫nmente aceptado consiste en considerar potenciales outliers a aquellas observaciones que caigan por fuera de  (Q1 - 1.5 RI) y (Q3 + 1.5 RI). La modificaci√≥n del gr√°fico consiste en extender los *whiskers* hasta las observaciones m√≠nima y m√°xima **que no sean puntos at√≠picos.** Los **outliers** se marcan en el gr√°fico como puntos separados de los *whiskers*.

![medidas_2.png](./imagenes/medidas_2.png)

Es importante remarcar que, en el gr√°fico anterior, la leyenda **‚ÄúValor m√°ximo‚Äù** se refiere al m√°ximo valor registrado u observado que es menor al l√≠mite de (Q3 + 1.5 RI).

Volviendo a nuestro dataset inicial, para construir el boxplot de los ingresos, podemos utilizar la librer√≠a `seaborn`:

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(data=data, x='bhih01', orient='h')
plt.xlabel('ingreso')
plt.show()
```

Y el diagrama resultante es:

![Untitled](./imagenes/Untitled%207.png)

### Consideraciones importantes sobre el ploteo de los *outliers*

En algunas situaciones, puede resultar de inter√©s construir la **versi√≥n cl√°sica del boxplot**, es decir, aquella que no plotea los potenciales valores at√≠picos del conjunto. Esto puede ser de utilidad, por ejemplo, cuando el tama√±o del dataset es considerablemente grande, situaci√≥n en la cual el boxplot modificado puede volverse completamente ilegible (muchos valores ser√°n indefectiblemente ploteados como potenciales *outliers,* lo que puede llevar, en casos extremos, a no ver la caja).

En `sns.boxplot()` puede setearse el par√°metro `showfliers = 'False'` para evitar que el gr√°fico resultante muestre los potenciales outliers. Sin embargo, esto altera la verdadera distribuci√≥n del dataset, ya que produce como resultado el equivalente a la versi√≥n ‚Äúmodificada‚Äù, con el agregado de que ya no se plotean los potenciales valores at√≠picos (¬°es como si se ‚Äúcortara‚Äù el boxplot y se eliminaran datos!). Por este motivo, no es una buena elecci√≥n si se quiere utilizar el boxplot como una herramienta gr√°fica para analizar caracter√≠sticas de la distribuci√≥n.

Frente a esto, una forma de construir el **boxplot cl√°sico** (el que no muestra potenciales *outliers*) **sin borrar datos ni alterar la verdadera distribuci√≥n de los mismos**, es modificar el par√°metro `whis`, extendiendo los whiskers hasta el menor y el mayor valor observado: `whis = [0,100]`.

### 

## Valores at√≠picos

Los valores at√≠picos son aquellos cuyo valor se encuentra alejado del grupo de datos. Supongamos que realizamos una encuesta para estudiar viajes al trabajo en el √°rea metropolitana de Rosario. Cuando estamos estudiando la distribuci√≥n encontramos que algunos viajes duran m√°s de 3 horas. Estos viajes, si bien ocurren, no son de nuestro inter√©s, porque alguien que tarda 3 horas o m√°s para llegar a su lugar de trabajo probablemente no viva dentro del √°rea que estamos estudiando y sus patrones de movilidad no nos interesen demasiado. Es m√°s, considerarlos puede llegar a sesgar nuestro estudio. Por esta raz√≥n, vamos a tener que pensar qu√© tratamiento se les va a dar, por ejemplo, eliminarlos. 

Existen varias formas de definir valores at√≠picos. Cuando graficamos un boxplot usando seaborn, como en el gr√°fico de arriba, como default, seaborn clasifica como outliers a todo punto que este fuera del rango $(Q_1 - 1.5*RIC, Q_{3} + 1.5*RIC)$. 

A continuaci√≥n mostramos un ejemplo de b√∫squeda de valores at√≠picos para la variable ‚Äúminutos durante los cuales realiz√≥ actividad f√≠sica intensa la semana pasada‚Äù. 

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

data = pd.read_csv('ENFR 2018 - Base usuario.txt', delimiter = '|')

# minutos actividad f√≠sica
sns.boxplot(data.biaf02_m)
plt.ylabel('Minutos por semana en actividad f√≠sica intensa')
```

![Untitled](./imagenes/Untitled%208.png)

En este caso vemos que por encima de 720 minutos (12 horas) seaborn comienza a plotear los valores at√≠picos. Sin embargo, es nuestra decisi√≥n si queremos incluirlos en el an√°lisis o no, o si queremos considerar otra medida para distinguir valores at√≠picos. Por ejemplo, puede ser que en nuestro estudio no queramos considerar deportistas profesionales. Entonces, deber√≠amos buscar un valor de minutos de entrenamiento semanal por el cual rechazamos a la observaci√≥n si esta se encuentra por encima de ese valor, por ejemplo 1000 minutos. 

## Tabla de Frecuencias

La¬†***frecuencia absoluta***¬†es una medida estad√≠stica que indica cu√°ntas veces se repite o se presenta un determinado valor en un conjunto de datos. La ***frecuencia relativa***, por su parte, es la frecuencia absoluta dividido entre el n√∫mero total de datos que componen el conjunto. Se la suele multiplicar por 100, de manera que exprese la proporci√≥n en la que se present√≥ un determinado dato en el dataset.

La **tabla de frecuencias** constituye una forma sencilla y efectiva para resumir la informaci√≥n de las observaciones de una variable de un dataset. Es una tabla que muestra la cantidad de veces que se presenta una determinado valor o rango de valores de una variable/atributo $X$. 

En el caso de una **variable cualitativa**, la tabla contiene las diferentes categor√≠as de la misma junto con la frecuencia absoluta de cada una de ellas en el dataset. Un ejemplo de este tipo de variable en la encuesta de Factores de Riesgo lo constituye la provincia a la que pertenece cada persona que respondi√≥ la encuesta. Entonces, podemos elaborar una tabla que nos indique cu√°ntas personas fueron encuestadas por provincia:

![Untitled](./imagenes/Untitled%209.png)

Para obtener la tabla de arriba usamos el siguiente c√≥digo:

```python
import pandas as pd

data = pd.read_csv('ENFR2018_baseusuario/ENFR 2018 - Base usuario.txt', delimiter = '|')

# Usando el diccionario de datos, mapeamos los c√≥digos a nombres para que la tabla
# sea m√°s f√°cil de entender
map_dict = {2: "Ciudad de Buenos Aires",
            6: " Buenos Aires",
            10: "Catamarca",
            14: "C√≥rdoba",
            18: "Corrientes",
            22: "Chaco",
            26: "Chubut",
            30: "Entre R√≠os",
            34: "Formosa",
            38: "Jujuy",
            42: "La Pampa",
            46: "La Rioja",
            50: "Mendoza",
            54: "Misiones",
            58: "Neuqu√©n",
            62: "R√≠o Negro",
            66: "Salta",
            70: "San Juan",
            74: "San Luis",
            78: "Santa Cruz",
            82: "Santa Fe",
            86: "Santiago del Estero",
            90: "Tucum√°n",
            94: "Tierra del Fuego",}
data['provincia'] = data['cod_provincia'].apply(lambda x: map_dict[x])

#Series.value_counts() es la encargada de generar la tabla de frecuencia. 
#El resto del c√≥digo es para embellecer el resultado
df = data.provincia.value_counts().reset_index()
df.rename(columns = {'index': 'provincia', 'provincia': 'frecuencia'}, inplace = True)
df.set_index('provincia', inplace = True)
```

![*Extracto de la tabla del ejemplo anterior*](./imagenes/Untitled%2010.png)

*Extracto de la tabla del ejemplo anterior*

A la tabla anterior, podemos agregarle una columna con las frecuencias relativas, para conocer qu√© proporci√≥n de la muestra pertenece a cada provincia. Este dato lo obtenemos dividiendo la frecuencia absoluta por el total de personas encuestadas. Podemos multiplicarlo por 100 para obtener los resultados en %.

```python
# Proporci√≥n de 0 a 1
df['proporci√≥n'] = df['frecuencia'] / df['frecuencia'].sum()

# Proporci√≥n de 0 a 100
df['proporci√≥n'] = df['frecuencia'] / df['frecuencia'].sum() * 100
```

![Untitled](./imagenes/Untitled%2011.png)

![Visualizaci√≥n gr√°fica de la frecuencia. Los valores fueron convertidos a % (de 0 100)](./imagenes/descarga.png)

Visualizaci√≥n gr√°fica de la frecuencia. Los valores fueron convertidos a % (de 0 100)

La tabla de arriba nos sirve para evaluar si la toma de muestra fue realizada correctamente, entre otras aplicaciones que podemos encontrarle. Cuando se hace un muestreo se plantea que la muestra debe representar de alguna forma a la poblaci√≥n. En nuestro caso esta representaci√≥n puede estar dada a trav√©s de la premisa que 

> La distribuci√≥n de personas encuestadas por provincia sea similar a la distribuci√≥n de personas por provincia a nivel pa√≠s.
> 

Para hacer esta comparaci√≥n, debemos generar adem√°s una tabla de frecuencias relativas de la poblaci√≥n por provincias. Luego comparamos las dos tablas de frecuencias relativas, la de la poblaci√≥n y la de la muestra, y evaluamos qu√© tanto ‚Äúse parecen‚Äù. 

<aside>
üí° La evaluaci√≥n de la correspondencia de la muestra con la poblaci√≥n es un proceso m√°s complejo en el cual se emplea m√°s de una variable.

</aside>

**Segmentaci√≥n**

La frecuencia se puede expresar de diferentes maneras, dependiendo del tipo de datos. Cuando trabajamos con **variables continuas**, en lugar de construir la tabla considerando los valores individuales que las mismas pueden asumir, es preciso realizar un agrupamiento previo en subintervalos (segmentaci√≥n). En el caso de minutos entrenados intensamente por semana podr√≠amos realizar la siguiente segmentaci√≥n por horas:

- Menos de una hora: [0-60)
- De una a dos horas: [60-120)
- De dos a tres horas: [120-180)
- De tres a cuatro horas: [180- 240)
- etc.

```python
data.biaf02_m.fillna(0, inplace = True)
data['segmento entrenamiento intenso'] = pd.cut(data.biaf02_m, [x for x in range(0, 4200, 60)], right = False)

df = data['segmento entrenamiento intenso'].value_counts()
df/df.sum() #frecuencias relativas

[0, 60)         0.784040
[180, 240)      0.044486
[120, 180)      0.039900
[60, 120)       0.035041
[240, 300)      0.022790
                  ...   
[1860, 1920)    0.000000
[1740, 1800)    0.000000
[1320, 1380)    0.000000
[1020, 1080)    0.000000
[4080, 4140)    0.000000
```

![descarga (1).png](./imagenes/descarga_(1).png)

### Frecuencia acumulada

En algunos casos, sobre todo cuando estudiamos variables continuas, resulta de utilidad realizar la tabla de frecuencia acumulada donde se suman las frecuencias, ya sea absolutas o relativas. Siguiendo con el ejemplo de arriba, podr√≠amos calcular la siguiente tabla de la que podemos extraer conclusiones como:

- El 10% de la poblaci√≥n realiza m√°s de 4 horas de entrenamiento intenso por semana.
- El 85% de la poblaci√≥n realiza menos de 3 horas de entrenamiento intenso por semana.

```python
df_relativo = (df/df.sum()).reset_index()
# Ordenamos los valores por segmento es decir [0,60), [60, 120), etc
df_relativo.sort_values('index', inplace = True)
df_relativo.set_index('index', inplace = True)

df_relativo.cumsum()

index	        segmento entrenamiento intenso
[0, 60)	      0.784040
[60, 120)	    0.819081
[120, 180)	  0.858981
[180, 240)	  0.903466
[240, 300)	  0.926257
...	...
[3840, 3900)	0.999966
[3900, 3960)	1.000000
[3960, 4020)	1.000000
[4020, 4080)	1.000000
[4080, 4140)	1.000000
```

<aside>
üí° Notar que para realizar la tabla de frecuencias acumuladas los valores deben estar ordenados por la variable en estudio, no por su frecuencia.

</aside>

## M√©tricas de correlaci√≥n

La correlaci√≥n es una medida estad√≠stica que describe el grado en que dos variables cambian o ‚Äúvar√≠an‚Äù conjuntamente.

### Covarianza

La covarianza es una medida estad√≠stica que brinda informaci√≥n acerca del grado en que dos variables var√≠an linealmente en forma conjunta. Suponiendo que se cuenta con un conjunto de n pares de observaciones de dos variables, X e Y, se calcula de la siguiente forma:

$$
S_{xy} = \frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
$$

Analicemos la ecuaci√≥n anterior:

- Si la relaci√≥n entre ambas variables es directa (a mayores valores de una variable le corresponden, en general, mayores valores de la otra, y viceversa), el producto $(x_i - \bar{x})(y_i - \bar{y})$ tender√° a ser positivo, dando como resultado una covariancia positiva.
- Si, por el contrario, la relaci√≥n entre ambas variables es inversa (a mayores valores de una variable le corresponden, en general, menores valores de la otra), el producto $(x_i - \bar{x})(y_i - \bar{y})$ tender√° a ser negativo, dando como resultado una covariancia negativa.

De esta forma, el **signo** de la covariancia indica si la asociaci√≥n entre las variables X e Y es positiva o negativa. Un ejemplo podr√≠a ser precio de chocolate ($Y$) y porcentaje de cacao ($X$). Probablemente, si estudiamos diferentes marcas de chocolate en el mercado y registramos el precio de venta por gramo y el porcentaje de cacao, vamos a observar que a medida que el porcentaje de cacao aumenta, tambi√©n lo hace el precio. Por lo tanto, si a partir de los datos recolectados calcul√°ramos la covariancia, esperar√≠amos obtener un valor positivo de la misma.

![positive.png](./imagenes/positive.png)

![negative.png](./imagenes/negative.png)

![cero.png](./imagenes/cero.png)

Cuando $X$ e $Y$ son estad√≠sticamente independientes, se puede demostrar que la covariancia es igual a 0. Sin embargo, lo opuesto no es necesariamente cierto: 2 variables pueden tener una covariancia igual a 0 y a√∫n as√≠ no ser estad√≠sticamente independientes. En este caso, $X$ e $Y$ podr√≠an tener una relaci√≥n con una forma distinta a la lineal y, de esta forma, no ser independientes.

La f√≥rmula de la covarianza  (poblaci√≥n) se define con la siguiente f√≥rmula:

$$
COV(X, Y) = E[(X - E[X])(Y - E[Y])]
$$

Donde: 

$E[X], E[Y]$ : es el valor esperado de la variable $X, Y$respectivamente. Es decir, la media poblacional.

 

<aside>
üí° Las $E$ que vemos en la ecuaci√≥n de arriba indican que se calcula el valor esperado de una variable aleatoria. El tema variables aleatorias se desarrolla con profundidad en PyE.

</aside>

A continuaci√≥n vamos a desglosar la f√≥rmula de arriba y vamos a desarrollar algunas intuiciones sobre la misma. 

$(X - E[X])$ calcula la distancia entre cada valor $X$y la media de $X$

$(Y - E[Y])$ calcula la distancia entre cada valor de $Y$y la media de $Y$

- Si, la distancia de $X$e $Y$a sus respectivas medias es ‚Äúgrande‚Äù y multiplicamos estas distancias, vamos a obtener un valor ‚Äúgrande‚Äù.
- Si ambas variables se alejan de sus medias en la misma direcci√≥n, es decir ambas son mayores o menores que las mismas, la multiplicaci√≥n va a ser positiva. Pero, si se alejan en sentidos opuestos, la multiplicaci√≥n va a ser negativa.
- Para que la covarianza adquiera un valor que se aleja de 0, ya sea positivo o negativo, la multiplicaci√≥n de las distancias de la mayor√≠a de los valores que asumen $X, Y$ deben estar en el mismo sentido. Si obtenemos multiplicaciones positivas y negativas balanceadas, el valor esperado de eso va a ser cero.
- Si $X,Y$se alejan ‚Äúpoco‚Äù de sus medias, no vamos a tener mucha variaci√≥n de por si y la covarianza va a ser ‚Äúchica‚Äù

**Limitaciones de la covarianza**

Aunque la covarianza es √∫til para identificar tendencias y relaciones en los datos, tambi√©n presenta algunas debilidades y limitaciones:

1. Escala de medida: La covarianza depende de las unidades de medida de las variables, lo que puede dificultar la interpretaci√≥n y comparaci√≥n de los valores de covarianza entre diferentes pares de variables. Por ejemplo, si las unidades de medida de una variable son metros y las de la otra variable son grados Celsius, la covarianza entre estas dos variables ser√° en unidades de metros por grados Celsius, lo que no es f√°cilmente interpretable.
2. Sensibilidad a cambios de escala: La covarianza es sensible a cambios de escala en las variables. Si una de las variables se multiplica por una constante, la covarianza tambi√©n se multiplicar√° por esa constante, lo que podr√≠a dar lugar a interpretaciones incorrectas de la relaci√≥n entre las variables.
3. Falta de normalizaci√≥n: A diferencia del coeficiente de correlaci√≥n de Pearson, la covarianza no est√° normalizada en un rango de -1 a 1. Esto dificulta la comparaci√≥n de la fuerza de las relaciones entre diferentes pares de variables, ya que no hay un valor m√°ximo o m√≠nimo absoluto para la covarianza.
4. Interpretaci√≥n de la fuerza de la relaci√≥n: La covarianza no proporciona informaci√≥n directa sobre la fuerza de la relaci√≥n entre las variables. Aunque una covarianza positiva indica una relaci√≥n directa y una covarianza negativa indica una relaci√≥n inversa, no se puede deducir cu√°n fuerte es esa relaci√≥n a partir del valor de la covarianza en s√≠.
5. Relaciones lineales √∫nicamente: La covarianza solo puede capturar relaciones lineales entre las variables. Si existe una relaci√≥n no lineal (por ejemplo, cuadr√°tica o exponencial) entre las variables, la covarianza no ser√° un indicador adecuado de esa relaci√≥n.

### Correlaci√≥n lineal de Pearson

El coeficiente de correlaci√≥n lineal de Pearson mide el grado de asociaci√≥n lineal que existe entre dos variables cuantitativas continuas. Se calcula de la siguiente manera:

$$
r = \frac{S_{xy}}{S_x S_y}
$$

donde $S_{xy}$ es la covariancia muestral entre X e Y, $S_{x}$ es la desviaci√≥n est√°ndar muestral de la variable X y $S_{y}$ es la desviaci√≥n est√°ndar muestral de la variable Y.

**Interpretaci√≥n**

Dado que se trata de una normalizaci√≥n de la covarianza, su valor est√° siempre comprendido entre -1 (correlaci√≥n lineal inversa perfecta) y 1 (correlaci√≥n lineal directa perfecta). 

- Un coeficiente de correlaci√≥n positivo indica una asociaci√≥n directa entre las variables (se mueven en la misma direcci√≥n). Por el contrario, un coeficiente de correlaci√≥n negativo es indicativo de una asociaci√≥n inversa entre las variables (se mueven en direcciones opuestas).
- Cuanto m√°s cercano a 1 sea, en valor absoluto, m√°s intensa es la correlaci√≥n lineal entre las variables.
- Un valor de r cercano a 0 no es necesariamente una evidencia de la falta de asociaci√≥n entre las variables, sino s√≥lo de la ausencia de una relaci√≥n lineal, ya que este coeficiente no dice nada acerca de la intensidad de asociaciones diferentes a las lineales. En estos casos su c√°lculo e interpretaci√≥n resultan inadecuados.
- Existe una tendencia a asumir, impl√≠cita o expl√≠citamente, que un alto valor de r implica causalidad, lo que constituye una interpretaci√≥n err√≥nea. El coeficiente de correlaci√≥n de Pearson es simplemente una medida de la fuerza de la asociaci√≥n lineal entre las dos variables.

![Ejemplos de correlaciones de variables con el m√©todo de Pearson ](./imagenes/Untitled%2012.png)

Ejemplos de correlaciones de variables con el m√©todo de Pearson 

$$
\rho_{X,Y} = {\frac {COV(X,Y)}{\sigma_{X}\sigma_{Y}}}
$$

**Ventajas frente a la covariancia**

1. Al estar acotado entre -1 y 1, el coeficiente de correlaci√≥n de Pearson proporciona informaci√≥n sobre la fuerza de la asociaci√≥n lineal entre las dos variables (cuanto m√°s cercano a 1, en valor absoluto, m√°s intensa es la asociaci√≥n lineal). 
2. Su valor es independiente de las unidades en las que est√©n medidas las variables y de los cambios de escala en los datos. Si las dos variables est√°n medidas en grados Celsius y las observaciones se convierten a grados Kelvin, el coeficiente de correlaci√≥n de Pearson calculado en esta nueva situaci√≥n ser√° id√©ntico al anterior.
3. El coeficiente de correlaci√≥n de Pearson es muy f√°cil de interpretar y de calcular.

**Supuestos y limitaciones**

La correlaci√≥n de Pearson asume que las variables poseen una distribuci√≥n normal conjunta y que la relaci√≥n entre las mismas es lineal. En relaci√≥n al primer punto, se trata de una exigencia que cobra especial importancia en el contexto de la inferencia estad√≠stica. Es generalmente aceptado que el coeficiente de correlaci√≥n lineal de Pearson puede utilizarse en pr√°cticamente todas las situaciones en las que nos enfrentemos a la necesidad de cuantificar el grado de asociaci√≥n lineal entre dos variables, independientemente de la forma de la distribuci√≥n conjunta. Por otra parte, en relaci√≥n al segundo punto, si existe una asociaci√≥n no lineal entre las variables (por ejemplo, cuadr√°tica o exponencial), el coeficiente de correlaci√≥n de Pearson no ser√° un indicador adecuado de esa relaci√≥n. 

Finalmente, es oportuno destacar que el coeficiente de correlaci√≥n de Pearson es bastante sensible a la presencia de valores at√≠picos en las observaciones, los cuales pueden distorsionar enormemente su valor y conducir a conclusiones equivocadas.

En la encuesta de Factores de Riesgo podr√≠amos pensar en estudiar las siguientes correlaciones:

| **X** | **Y** | **Valor** |
| --- | --- | --- |
| Edad | Minutos entrenamiento intenso | - 0.16 |
| Ingresos | Minutos entrenamiento intenso | 0.04 |
| Edad | Minutos de caminata | - 0.03 |
| Ingresos | Edad cuando fum√≥ por primera vez | - 0.03 |

Hay muchas maneras de calcular el coeficiente de variaci√≥n en Python. Usando pandas podemos hacerlo de la siguiente forma:

```python
# bhch04 es la edad y biaf02_m es la cantidad de minutos entrenados de forma intensa 
# en la semana. El m√©todo por default es `pearson`
data.bhch04.corr(data.biaf02_m, method = 'pearson')
```

### Correlaci√≥n de Spearman

Para poder entender la correlaci√≥n de Spearman primero necesitamos revisar el ranking

**Ranking**

Un ranking es una lista ordenada de elementos o entidades en funci√≥n de alg√∫n criterio de clasificaci√≥n espec√≠fico. El objetivo del ranking es proporcionar una forma de clasificar y comparar los elementos o entidades de inter√©s.

Los rankings se utilizan com√∫nmente en una variedad de campos, como deportes, negocios, educaci√≥n, ciencias, entre otros. Por ejemplo, en los deportes, los rankings se utilizan para clasificar a los equipos o jugadores en funci√≥n de su rendimiento en la temporada actual o en temporadas anteriores. En los negocios, los rankings se utilizan para clasificar a las empresas en funci√≥n de su tama√±o, ingresos, beneficios, entre otros criterios. En la educaci√≥n, los rankings se utilizan para clasificar a las universidades en funci√≥n de su reputaci√≥n, calidad acad√©mica, entre otros criterios.

Los rankings pueden basarse en diferentes criterios de clasificaci√≥n, como puntuaciones, tiempos, ingresos, tama√±o, popularidad, entre otros. Es importante tener en cuenta que el criterio de clasificaci√≥n utilizado para crear un ranking puede afectar la posici√≥n de los elementos o entidades en la lista.

Los rankings pueden tener un impacto significativo en la percepci√≥n p√∫blica y el √©xito de los elementos o entidades clasificados. Sin embargo, es importante recordar que los rankings no siempre son una medida precisa o completa del rendimiento o calidad de los elementos o entidades clasificados, y que deben interpretarse con precauci√≥n.

El ranking establece una relaci√≥n entre los valores que asume una variable de forma tal que sabemos el orden de la misma sin que nos importe el valor en si. Por ejemplo, la satisfacci√≥n con un producto podr√≠a ser rankeada de esta forma

- üòç : 1
- üòÄ : 2
- üòë : 3
- ‚òπÔ∏è : 4
- ü§¨ : 5

Otro ejemplo podr√≠a ser rankear la variable `edad` de nuestro ejemplo. Para eso ordenamos las edades y le asignamos el valor 1 a la edad m√°s grande y la cantidad de personas encuestadas al valor m√°s chico. Luego, las edades intermedias reciben el correspondiente rango intermedio. Un ejemplo se ver√≠a as√≠:

[18, 20, 22, 30, 34, 35, 40, 47, 50, 51]

[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]

Lo que hicimos arriba es posible porque en nuestro ejemplo las edades no se repiten, lo cual es poco real. Cuando los valores se repiten esto se llama empate y tenemos que buscar una estrategia para asignarles el ranking. Supongamos que tenemos las siguientes edades: 

[18, 20, 22, 22, 30, 34, 35, 35, 35, 40, 47, 50, 51]

A continuaci√≥n listamos algunas estrategias para rankearlas, [fuente Wikipedia](https://en.wikipedia.org/wiki/Ranking). 

**Ranking de competici√≥n est√°ndar**

En la clasificaci√≥n de competici√≥n, los elementos que se comparan y resultan iguales reciben el mismo n√∫mero de clasificaci√≥n y luego se deja un espacio en los n√∫meros de clasificaci√≥n. La cantidad de n√∫meros de clasificaci√≥n que se omiten en este espacio es uno menos que la cantidad de elementos que se compararon como iguales. De manera equivalente, el n√∫mero de clasificaci√≥n de cada elemento es 1 m√°s la cantidad de elementos clasificados por encima de √©l. Esta estrategia de clasificaci√≥n se adopta frecuentemente en competencias, ya que significa que si dos (o m√°s) competidores empatan en una posici√≥n en la clasificaci√≥n, la posici√≥n de todos aquellos clasificados por debajo de ellos no se ve afectada (es decir, un competidor solo queda en segundo lugar si exactamente una persona obtiene una puntuaci√≥n mejor que ellos, tercero si exactamente dos personas obtienen una puntuaci√≥n mejor que ellos, cuarto si exactamente tres personas obtienen una puntuaci√≥n mejor que ellos, etc.).

En nuestro ejemplo, obtendr√≠amos este ranking: 

[1, 2, 3, 3, 5, 6, 7, 7, 7, 10, 11, 12, 13]

**Ranking de competici√≥n modificado**

A veces, la clasificaci√≥n de competici√≥n se realiza dejando los espacios en los n√∫meros de clasificaci√≥n antes de los conjuntos de elementos con clasificaci√≥n igual (en lugar de despu√©s de ellos, como en la clasificaci√≥n de competici√≥n est√°ndar). La cantidad de n√∫meros de clasificaci√≥n que se omiten en este espacio sigue siendo uno menos que la cantidad de elementos que se compararon como iguales. De manera equivalente, el n√∫mero de clasificaci√≥n de cada elemento es igual a la cantidad de elementos clasificados iguales o por encima de √©l. Esta clasificaci√≥n garantiza que un competidor solo queda en segundo lugar si obtiene una puntuaci√≥n m√°s alta que todos menos uno de sus oponentes, tercero si obtiene una puntuaci√≥n m√°s alta que todos menos dos de sus oponentes, etc. El ranking nos quedar√≠a de este modo: 

[1, 2, 4, 4, 5, 6, 9, 9, 9, 10, 11, 12, 13]

**Ranking denso**

En la clasificaci√≥n densa, los elementos que se comparan de manera igual reciben el mismo n√∫mero de clasificaci√≥n y los siguientes elementos reciben el n√∫mero de clasificaci√≥n inmediatamente posterior. De manera equivalente, el n√∫mero de clasificaci√≥n de cada elemento es 1 m√°s la cantidad de elementos clasificados por encima de √©l que son distintos con respecto al orden de clasificaci√≥n. El ranking nos quedar√≠a de este modo: 

[1, 2, 3, 3, 4, 5, 6, 6, 6, 7, 8, 9, 10]

**Ranking ordinal**

En la clasificaci√≥n ordinal, todos los elementos reciben n√∫meros ordinales distintos, incluidos los elementos que se comparan como iguales. La asignaci√≥n de n√∫meros ordinales distintos a elementos que se comparan igual se puede hacer de manera aleatoria o arbitraria, pero generalmente es preferible utilizar un sistema que sea arbitrario pero consistente, ya que esto proporciona resultados estables si la clasificaci√≥n se realiza varias veces. Un ejemplo de un sistema arbitrario pero consistente ser√≠a incorporar otros atributos al orden de clasificaci√≥n (como el orden alfab√©tico del nombre del competidor) para garantizar que no haya dos elementos que coincidan exactamente.
El ranking nos quedar√≠a de este modo:

[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]

**Ranking fraccional**

Los elementos que se comparan igual reciben el mismo n√∫mero de clasificaci√≥n, que es la media de lo que tendr√≠an bajo las clasificaciones ordinales; de manera equivalente, el n√∫mero de clasificaci√≥n de 1 m√°s la cantidad de elementos clasificados por encima de √©l m√°s la mitad de la cantidad de elementos iguales a √©l. Esta estrategia tiene la propiedad de que la suma de los n√∫meros de clasificaci√≥n es la misma que en la clasificaci√≥n ordinal. El ranking nos quedar√≠a de este modo:
[1, 2, 3.5, 3.5, 5, 6, 8, 8, 8, 10, 11, 12, 13]

Con `pandas`, podemos rankear una serie con diferentes m√©todos de ranking: 

```python
import pandas as pd

data = [18, 20, 22, 22, 30, 34, 35, 35, 35, 40, 47, 50, 51]
serie = pd.Series(data)

# min: **Ranking de competici√≥n est√°ndar**
# max: **Ranking de competici√≥n modificado**
# dense: **Ranking denso**
# average: **Ranking fraccional**
# first: **Ranking ordinal**
ranking_methods = ['average', 'min', 'max', 'dense', 'first']

for method in ranking_methods:
    ranked = serie.rank(method=method)
    print(f"M√©todo de Ranking: {method}")
    print(ranked.tolist())
    print("\n")

"""
Ranking method: average
[1.0, 2.0, 3.5, 3.5, 5.0, 6.0, 8.0, 8.0, 8.0, 10.0, 11.0, 12.0, 13.0]

Ranking method: min
[1.0, 2.0, 3.0, 3.0, 5.0, 6.0, 7.0, 7.0, 7.0, 10.0, 11.0, 12.0, 13.0]

Ranking method: max
[1.0, 2.0, 4.0, 4.0, 5.0, 6.0, 9.0, 9.0, 9.0, 10.0, 11.0, 12.0, 13.0]

Ranking method: dense
[1.0, 2.0, 3.0, 3.0, 4.0, 5.0, 6.0, 6.0, 6.0, 7.0, 8.0, 9.0, 10.0]

Ranking method: first
[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0]
"""
```

**Correlaci√≥n de Spearman**

La correlaci√≥n de Spearman se define como el coeficiente de correlaci√≥n de Pearson para las variables rankeadas. Esto nos permite evaluar si existe una relaci√≥n monot√≥nica entre dos variables, es decir si crecen/decrecen juntas, sin importar si la relaci√≥n es lineal o no, a diferencia de Pearson. Adem√°s el valor obtenido es menos sensible a valores at√≠picos que el coeficiente de Pearson. 

La f√≥rmula general es: 

$$
\rho = {\frac{COV(R(X), R(Y))}{\sigma_{R(X)}\sigma_{R(Y)}}}
$$

La correlaci√≥n de Spearman tiene una relaci√≥n directa con los rankings (rangos) porque se basa en los rangos de los datos en lugar de sus valores originales. Al calcular la correlaci√≥n de Spearman, convertimos los valores de las dos variables en rangos antes de evaluar la asociaci√≥n entre ellas. Esta caracter√≠stica hace que la correlaci√≥n de Spearman sea especialmente √∫til para analizar datos ordinales, donde la informaci√≥n m√°s importante es el orden o la clasificaci√≥n de las observaciones en lugar de sus valores num√©ricos exactos.

Cuando comparamos dos conjuntos de rankings, la correlaci√≥n de Spearman nos indica qu√© tan bien estos rankings concuerdan entre s√≠. Si los rankings de las dos variables son id√©nticos, la correlaci√≥n de Spearman ser√° 1, lo que indica una relaci√≥n monot√≥nica creciente perfecta. Si los rankings son exactamente opuestos, la correlaci√≥n de Spearman ser√° -1, lo que indica una relaci√≥n monot√≥nica decreciente perfecta. Un valor cercano a 0 sugiere que no hay una relaci√≥n monot√≥nica evidente entre los rankings de las dos variables.

![Ejemplos de variables monot√≥nicas y no monot√≥nicas](./imagenes/Untitled%2013.png)

Ejemplos de variables monot√≥nicas y no monot√≥nicas

Antes de calcular la correlaci√≥n de Spearman, debemos convertir los valores de las variables en rangos. Aqu√≠ se detallan los pasos para convertir los datos en rangos:

1. Ordenar los valores de cada variable (X e Y) en orden ascendente, de menor a mayor.
2. Asignar un rango a cada valor en funci√≥n de su posici√≥n en la lista ordenada. El valor m√°s peque√±o recibir√° el rango 1, el siguiente valor m√°s peque√±o recibir√° el rango 2 y as√≠ sucesivamente.
3. En caso de empates (valores iguales), utilizar el m√©todo de ranking ordinal.

Repita estos pasos para ambas variables, X e Y. Al final, tendremos dos conjuntos de datos transformados en rangos, que se pueden utilizar para calcular el coeficiente de correlaci√≥n de rango de Spearman (œÅ).

Aqu√≠ hay un ejemplo para ilustrar el proceso:

Suponga que tiene dos variables, X e Y, con los siguientes valores:

X: [10, 25, 30, 45, 55]
Y: [2, 8, 10, 12, 22]

Primero, ordene los valores de cada variable y as√≠gneles rangos:

X: 10 (R1 = 1), 25 (R2 = 2), 30 (R3 = 3), 45 (R4 = 4), 55 (R5 = 5)
Y: 2 (R1 = 1), 8 (R2 = 2), 10 (R3 = 3), 12 (R4 = 4), 22 (R5 = 5)

Luego la f√≥rmula es:

œÅ = 1 - (6 * Œ£d^2) / (n * (n^2 - 1))

$$
\rho = \frac{1-(6\times \sum d^2)}{(n\times(n^2-1)}
$$

Donde:

- n es el n√∫mero de observaciones (pares de datos)
- Œ£d^2 es la suma de las diferencias al cuadrado de los rangos.

El coeficiente de correlaci√≥n de Spearman (œÅ) var√≠a entre -1 y 1, donde:

- œÅ = 1 indica una relaci√≥n monot√≥nica creciente perfecta.
- œÅ = -1 indica una relaci√≥n monot√≥nica decreciente perfecta.
- œÅ ‚âà 0 sugiere que no hay una relaci√≥n monot√≥nica evidente entre las variables.

Si calculamos el coeficiente de Spearman con Pandas, la librer√≠a se encarga de facilitar el proceso de generaci√≥n de rankings. El coeficiente se puede calcular de la misma forma que el coeficiente de correlaci√≥n de Pearson pero cambiando el m√©todo

```python
data.bhch04.corr(data.biaf02_m, method = 'spearman')
```

Aqu√≠ vemos el valor del coeficiente con diferentes funciones como ejemplo ([fuente](https://stackabuse.com/calculating-spearmans-rank-correlation-coefficient-in-python-with-pandas/)):

![Untitled](./imagenes/Untitled%2014.png)

![Untitled](./imagenes/Untitled%2015.png)

![Untitled](./imagenes/Untitled%2016.png)

## Matriz de Covarianza

La matriz de covarianza es una matriz cuadrada donde se muestra el coeficiente de covarianza entre m√∫ltiples variables. En el ejemplo de abajo mostramos la matriz de covarianza entre edad (bhch04), minutos semanales de actividad intensa (biaf02_m), ingreso (bhih01), minutos de caminata semanal (biaf06_m) y edad en que comenz√≥ a fumar (bita02).

![Untitled](./imagenes/Untitled%2017.png)

Esta matriz nos permite ver de forma r√°pida si existe alguna relaci√≥n lineal entre las variables y nos puede servir en el futuro como gu√≠a para selecci√≥n de atributos para nuestros modelos. 

Para construir la tabla de arriba usamos Pandas de la siguiente forma:

```python
data[['bhch04', 'biaf02_m', 'bhih01', 'biaf06_m', 'bita02']].cov()
```

## Matriz de Correlaci√≥n

La matriz de correlaci√≥n es similar a la anterior pero en lugar de mostrar el coeficiente de covarianza nos muestra el de correlaci√≥n. 

```python
#en este caso el m√©todo por default es el de pearson
data[['bhch04', 'biaf02_m', 'bhih01', 'biaf06_m', 'bita02']].corr()
```

![Untitled](./imagenes/Untitled%2018.png)

La diagonal siempre tiene el valor de 1 puesto que es la correlaci√≥n de una variable consigo misma. Tanto en esta matriz como en la anterior, solo nos interesa mirar la parte que est√° por encima o debajo de la diagonal, puesto que estas matrices son siempre sim√©tricas. 

Una forma de visualizar r√°pidamente la correlaci√≥n que existe entre un grupo de variables es  imprimiendo la matriz de correlaci√≥n con colores:

```python
sns.heatmap(data[['bhch04', 'biaf02_m', 'bhih01', 'biaf06_m', 'bita02']].corr(), annot=True)
```

![Untitled](./imagenes/Untitled%2019.png)

## Temas avanzados

## M√©tricas de Distancia y Similaridad

Son m√©tricas que nos a permiten definir cu√°nto se parecen dos puntos de datos, de forma similar a lo que vimos en la Unidad 2 con la similaridad de Levensthein on Jaro-Winkler. Existen diferentes formas de medir la similaridad y la selecci√≥n de la misma depende del an√°lisis que se est√© realizando. A continuaci√≥n listamos algunas de ellas.

### **Distancia Euclideana**

La definici√≥n formal es que la distancia euclideana es la distancia entre dos puntos en un [espacio eucl√≠deo](https://es.wikipedia.org/wiki/Espacio_eucl%C3%ADdeo). La forma de visualizar esto es pensarlo como la distancia en linea recta entre dos puntos en el espacio. 

Si hablamos de un espacio de n dimensiones y queremos calcular la distancia entre los puntos P y Q, entonces:

$$
d_{E(P,Q)}= \sqrt{\sum_{i = 1}^{n}(p_i - q_i)^2}
$$

![Untitled](./imagenes/Untitled%2020.png)

**Desventajas**
Aunque es una medida de distancia com√∫n, la distancia euclidiana no es una escala invariante, lo que significa que las distancias calculadas pueden estar sesgadas seg√∫n las unidades de las entidades. Por lo general, uno necesita normalizar los datos antes de usar esta medida de distancia.

Adem√°s, a medida que aumenta la dimensionalidad de sus datos, la distancia euclidiana se vuelve menos √∫til. 

**Casos de uso**
La distancia euclidiana funciona muy bien cuando tiene datos de baja dimensi√≥n y es importante medir la magnitud de los vectores. M√©todos como [kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) y [HDBSCAN](https://en.wikipedia.org/wiki/DBSCAN#Extensions) muestran excelentes resultados listos para usar si se usa la distancia euclidiana en datos de baja dimensi√≥n.

### **Distancia Manhattan**

La distancia de Manhattan se calcula como la suma de la diferencia absoluta entre los componentes de dos vectores P y Q 

$$
d_{M(P,Q)} = \sum_{i = 1}^{n}|p_i - q_i|
$$

La distancia de Manhattan es la norma L1 de un vector mientras que la euclideana es la norma L2. 

![Distancia Manhattan contra distancia Euclidiana: Las l√≠neas roja, azul y amarilla tienen la misma longitud (12) en las geometr√≠as Euclidiana y taxicab. En la geometr√≠a Euclidiana, la l√≠nea verde tiene longitud 6√ó‚àö2 ‚âà 8.48, y es el √∫nico camino m√°s corto. En la geometr√≠a taxicab, la l√≠nea verde tiene longitud 12, por lo que no es m√°s corta que los otros caminos.](./imagenes/Untitled%2021.png)

Distancia Manhattan contra distancia Euclidiana: Las l√≠neas roja, azul y amarilla tienen la misma longitud (12) en las geometr√≠as Euclidiana y taxicab. En la geometr√≠a Euclidiana, la l√≠nea verde tiene longitud 6√ó‚àö2 ‚âà 8.48, y es el √∫nico camino m√°s corto. En la geometr√≠a taxicab, la l√≠nea verde tiene longitud 12, por lo que no es m√°s corta que los otros caminos.

**Desventajas**
Aunque la distancia de Manhattan parece funcionar bien para datos de alta dimensi√≥n, es una medida algo menos intuitiva que la distancia euclidiana, especialmente cuando se usa en datos de alta dimensi√≥n.

Adem√°s, es m√°s probable que proporcione un valor de distancia m√°s alto que la distancia euclidiana, ya que no ofrece el camino m√°s corto posible. Esto no necesariamente da problemas, pero es algo que debe tener en cuenta.

**Casos de uso**
Cuando su conjunto de datos tiene atributos discretos y/o binarios, Manhattan parece funcionar bastante bien, ya que tiene en cuenta las rutas que, de manera realista, podr√≠an tomarse dentro de los valores de esos atributos. Tomar la distancia euclidiana, por ejemplo, crear√≠a una l√≠nea recta entre dos vectores cuando en realidad esto podr√≠a no ser posible.

### **Similaridad de Coseno**

Mide el √°ngulo entre dos vectores en un espacio de varias dimensiones. Es una m√©trica popular para la comparaci√≥n de documentos en procesamiento de lenguaje natural y sistemas de recomendaci√≥n.

Sabemos que el producto punto entre dos vectores es igual al coseno del √°ngulo entre ellos por la longitud de los mismos

$$
u.v = cos(\theta) |u||v|
$$

Con esto se define la similitud coseno de la siguiente forma

$$
cos(\theta) = \frac{u.v}{|u||v|}
$$

Mirando la gr√°fica de la funci√≥n coseno podemos decir que:

- Cuando $\theta$ es igual a 0, el coseno es igual a 1. En nuestro caso, los dos vectores coinciden perfectamente
- Cuando $\theta$ es igual a $\pi/2$ , el coseno es igual a 0. En nuestro caso, los dos vectores son ortogonales
- Cuando $\theta$ es igual a $\pi$, el coseno es igual a -1. En nuestro caso, los dos vectores se encuentran en posici√≥n diametralmente opuesta

![Untitled](./imagenes/Untitled%2022.png)

<aside>
üí° Notar que la magnitud de los vectores no entra en juego para esta similaridad

</aside>

![Untitled](./imagenes/Untitled%2023.png)

**Desventajas**
Una de las principales desventajas de la similitud del coseno es que no se tiene en cuenta la magnitud de los vectores, sino simplemente su direcci√≥n. En la pr√°ctica, esto significa que las diferencias de valores no se tienen plenamente en cuenta. Si toma un sistema de recomendaci√≥n, por ejemplo, la similitud del coseno no tiene en cuenta la diferencia en la escala de calificaci√≥n entre diferentes usuarios.

**Casos de uso**
Usamos la similitud del coseno a menudo cuando tenemos datos de alta dimensi√≥n y cuando la magnitud de los vectores no es importante. Para los an√°lisis de texto, esta medida se utiliza con bastante frecuencia cuando los datos se representan mediante recuentos de palabras. Por ejemplo, cuando una palabra aparece con m√°s frecuencia en un documento que en otro, esto no significa necesariamente que un documento est√© m√°s relacionado con esa palabra. Puede darse el caso de que los documentos tengan longitudes desiguales y la magnitud del conteo sea de menor importancia. Entonces, podemos usar mejor la similitud del coseno que no tiene en cuenta la magnitud.

### **Distancia de Mahalanobis**

En estad√≠stica, la distancia de Mahalanobis es una medida de distancia introducida por Mahalanobis en 1936. Su utilidad radica en que es una forma de determinar la similitud entre dos variables aleatorias multidimensionales. Se diferencia de la distancia eucl√≠dea en que tiene en cuenta la correlaci√≥n entre las variables aleatorias.

![La distancia de Mahalanobis (MD) es una m√©trica de distancia efectiva que encuentra la distancia entre el punto y la distribuci√≥n. Funciona con bastante eficacia en datos multivariados porque utiliza una matriz de covarianza de variables para encontrar la distancia entre los puntos de datos y el centro. Esto significa que MD detecta valores at√≠picos en funci√≥n del patr√≥n de distribuci√≥n de los puntos de datos, a diferencia de la distancia euclidiana.](./imagenes/Untitled%2024.png)

La distancia de Mahalanobis (MD) es una m√©trica de distancia efectiva que encuentra la distancia entre el punto y la distribuci√≥n. Funciona con bastante eficacia en datos multivariados porque utiliza una matriz de covarianza de variables para encontrar la distancia entre los puntos de datos y el centro. Esto significa que MD detecta valores at√≠picos en funci√≥n del patr√≥n de distribuci√≥n de los puntos de datos, a diferencia de la distancia euclidiana.

En la siguientes figuras, el punto negro representa el centroide de la distribuci√≥n. En la gr√°fica de la izquierda, tenemos una distribuci√≥n no correlacionada. En ese caso, los valores at√≠picos ser√≠an los m√°s alejados del centroide y se podr√≠a usar la distancia euclidiana para detectarlos. En el caso de la derecha, cuando las variables est√°n correlacionadas, tiene m√°s sentido usar Mahalanobis, ya que tiene en cuenta la correlaci√≥n entre ambas variables. All√≠ el punto 2 ser√≠a un outlier, mientras que el punto 1 es parte de la distribuci√≥n. 

![Untitled](./imagenes/Untitled%2025.png)

La distancia de Mahalanobis mide la distancia entre un punto P y el centroide de una distribuci√≥n D y es igual a la distancia euclideana cuando las variables no est√°n correlacionadas. 

Se define como:

$$
d_{M(x, \mu, D)} = \sqrt{(x - \mu)^T S^{-1}(x - \mu)}
$$

Y cuando nos interesa saber la distancia entre dos puntos, en vez de un punto y el centroide de los datos:

$$
d_{M(x,y, D)} = \sqrt{(x - y)^T S^{-1}(x - y)}
$$

Donde

$x, y$: puntos en el espacio

$\mu$: media de la distribuci√≥n D

$S$: matriz de covarianza

Crearemos un conjunto de datos que muestre el puntaje del examen de 20 estudiantes junto con la cantidad de horas que dedicaron a estudiar, la cantidad de ex√°menes de preparaci√≥n que tomaron y su calificaci√≥n actual en el curso::

```python
import numpy as np
import pandas as pd 
import scipy as stats
from scipy.stats import chi2

data = {'score': [91, 93, 72, 87, 86, 73, 68, 87, 78, 99, 95, 76, 84, 96, 76, 80, 83, 84, 73, 74],
        'hours': [16, 6, 3, 1, 2, 3, 2, 5, 2, 5, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4],
        'prep': [3, 4, 0, 3, 4, 0, 1, 2, 1, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2],
        'grade': [70, 88, 80, 83, 88, 84, 78, 94, 90, 93, 89, 82, 95, 94, 81, 93, 93, 90, 89, 89]
        }

df = pd.DataFrame(data,columns=['score', 'hours', 'prep','grade'])

# Crear funci√≥n para calcular la distancia de Mahalanobis
def mahalanobis(x=None, data=None, cov=None):

    x_mu = x - np.mean(data)
    if not cov:
        cov = np.cov(data.values.T)
    inv_covmat = np.linalg.inv(cov)
    left = np.dot(x_mu, inv_covmat)
    mahal = np.dot(left, x_mu.T)
    return mahal.diagonal()

# Crea una nueva columna en el marco de datos que contiene 
# la distancia de Mahalanobis para cada fila
df['mahalanobis'] = mahalanobis(x=df, data=df[['score', 'hours', 'prep', 'grade']])

# Calcular el valor p para cada distancia mahalanobis
df['p'] = 1 - chi2.cdf(df['mahalanobis'], 3)

# Mostrar valores p para las primeras cinco filas en el marco de datos
df.head()

"""
  score	hours	prep	grade	mahalanobis	p
0	91	16	3	70	16.501963	0.000895
1	93	6	4	88	2.639286	0.450644
2	72	3	0	80	4.850797	0.183054
3	87	1	3	83	5.201261	0.157639
4	86	2	4	88	3.828734	0.280562
"""
```

La interpretaci√≥n de los valores p es la siguiente:

- Un valor p cercano a 1 indica que la probabilidad de que la observaci√≥n sea un valor t√≠pico dentro de la distribuci√≥n es alta, por lo que no se considera un valor at√≠pico.
- Un valor p cercano a 0 indica que la probabilidad de que la observaci√≥n sea un valor t√≠pico dentro de la distribuci√≥n es baja, lo que sugiere que podr√≠a ser un valor at√≠pico. Generalmente, se considera que una observaci√≥n con un valor p inferior a 0.001 es un valor at√≠pico. En este ejemplo, vemos un valor at√≠pico en el primer registro.

**Desventajas**

Suposici√≥n de normalidad: La distancia de Mahalanobis supone que los datos siguen una distribuci√≥n normal multivariante. Si esta suposici√≥n no se cumple, la distancia de Mahalanobis podr√≠a no ser apropiada o requerir√≠a ajustes en la metodolog√≠a.

Sensibilidad a la matriz de covarianza: La distancia de Mahalanobis depende de la matriz de covarianza, y el c√°lculo de su inversa puede ser num√©ricamente inestable en algunos casos. Esto puede ser un problema en conjuntos de datos de alta dimensionalidad o cuando hay multicolinealidad (variables altamente correlacionadas).

No apropiada para distribuciones no lineales: La distancia de Mahalanobis no es adecuada para conjuntos de datos con relaciones no lineales entre las variables, ya que solo considera las correlaciones lineales.

**Casos de uso**

Detecci√≥n de valores at√≠picos: La distancia de Mahalanobis es √∫til para identificar valores at√≠picos en un conjunto de datos multivariante, ya que considera la correlaci√≥n entre las variables y la variabilidad de cada variable. Los puntos con distancias de Mahalanobis grandes pueden ser considerados como valores at√≠picos.

Clasificaci√≥n y reconocimiento de patrones: La distancia de Mahalanobis puede utilizarse en problemas de clasificaci√≥n y reconocimiento de patrones, donde se intenta asignar un objeto desconocido a una de varias clases predefinidas. La distancia de Mahalanobis puede ser utilizada como m√©trica de similitud para determinar la cercan√≠a entre un objeto desconocido y los objetos en cada clase.

An√°lisis de conglomerados (clustering): La distancia de Mahalanobis puede ser utilizada en algoritmos de clustering para medir la similitud entre puntos en un espacio multivariante. Algoritmos como K-means o agrupamiento jer√°rquico pueden utilizar la distancia de Mahalanobis en lugar de la distancia euclidiana para tener en cuenta la correlaci√≥n entre variables y la variabilidad de cada variable.

### **Distancia de Jaccard**

El coeficiente de similitud de Jaccard o √≠ndice de Jaccard mide la similitud entre dos conjuntos de muestras y fue concebido con la intenci√≥n de comparar los tipos de flores presentes en un ecosistema de la cuenca de un r√≠o, con los tipos presentes en las regiones aleda√±as. Se define como la relaci√≥n entre el tama√±o de la intersecci√≥n de ambos conjuntos y el tama√±o de la uni√≥n:

$$
sim_{J(A,B)} = \frac{|A\cap B|}{|A \cup B|}
$$

La¬†*distancia de Jaccard*¬†mide la disimilitud entre dos conjuntos de muestras y se define como el complemento del coeficiente de Jaccard:

$$
J (A,B) = 1 - sim_{J(A,B)}
$$

El coeficiente de Jaccard toma valores entre 0 y 1, siendo 1 la coincidencia perfecta entre ambos conjuntos, cuando la intersecci√≥n es igual a la uni√≥n

Esta distancia puede utilizarse en an√°lisis de texto: La distancia de Jaccard se utiliza para comparar documentos o textos en funci√≥n de la similitud entre sus conjuntos de palabras, caracteres o n-gramas, o puede ser √∫til tambi√©n en sistemas de recomendaci√≥n, para medir la similitud entre usuarios o elementos en funci√≥n de los conjuntos de caracter√≠sticas, preferencias o comportamientos de los usuarios.

**Desventajas**

La distancia de Jaccard puede verse afectada por diferencias significativas en el tama√±o de los conjuntos que se comparan. Por ejemplo, si un conjunto es mucho m√°s grande que otro, la distancia de Jaccard puede ser alta incluso si hay una superposici√≥n considerable entre los conjuntos.

No apta para datos num√©ricos continuos: La distancia de Jaccard est√° dise√±ada para comparar conjuntos y, por lo tanto, no es adecuada para trabajar con datos num√©ricos continuos. Para datos continuos, es necesario utilizar otras medidas de distancia, como la distancia euclidiana o la distancia de Mahalanobis.

No considera la estructura interna de los datos: La distancia de Jaccard solo tiene en cuenta la presencia o ausencia de elementos en los conjuntos, pero no considera la estructura interna o las relaciones entre los elementos dentro de los conjuntos. En situaciones donde la estructura interna de los datos es importante, podr√≠an ser m√°s adecuadas otras medidas de similitud o t√©cnicas de an√°lisis.

Sensibilidad a la presencia de elementos irrelevantes: La distancia de Jaccard puede verse afectada por la presencia de elementos irrelevantes o ruidosos en los conjuntos. Estos elementos pueden aumentar la distancia de Jaccard entre dos conjuntos que, de lo contrario, ser√≠an muy similares.

**Casos de uso**

La distancia de Jaccard se utiliza a menudo en el an√°lisis de texto para comparar la similitud entre documentos. Por ejemplo, se pueden comparar documentos en funci√≥n de las palabras o t√©rminos que contienen, y luego agruparlos o clasificarlos seg√∫n su similitud.

Comparaci√≥n de comunidades biol√≥gicas: En ecolog√≠a y biolog√≠a, la distancia de Jaccard se utiliza a menudo para comparar la similitud entre comunidades biol√≥gicas, como la presencia de especies en diferentes h√°bitats o ecosistemas. Tambi√©n se puede aplicar en el an√°lisis de datos gen√©ticos para comparar la similitud entre diferentes genomas, especialmente en el contexto de datos de secuenciaci√≥n de ADN.

Sistemas de recomendaci√≥n: La distancia de Jaccard puede ser √∫til en sistemas de recomendaci√≥n para medir la similitud entre usuarios o elementos en funci√≥n de sus caracter√≠sticas binarias o categ√≥ricas. Por ejemplo, se puede utilizar para comparar la similitud entre usuarios en funci√≥n de las pel√≠culas que han visto o los productos que han comprado. Tambi√©n en el an√°lisis de redes sociales, la distancia de Jaccard se utiliza a menudo para comparar la similitud entre los miembros de una red en funci√≥n de sus conexiones o intereses compartidos.

### **Distancias entre palabras**

Pueden consultar a la unidad 2 donde estudiamos distancia de Levensthein y de Jaro-Winkler

[**KPI Est√°ndares**](https://www.notion.so/KPI-Est-ndares-86f5927d6c7745b2ae7fdee69d4decee?pvs=21)